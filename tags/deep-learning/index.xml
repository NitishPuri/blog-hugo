<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>deep-learning on Blog - Nitish Puri</title><link>https://nitishpuri.github.io/blog-hugo/tags/deep-learning/</link><description>Recent content in deep-learning on Blog - Nitish Puri</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 02 Jan 2018 00:00:00 +0000</lastBuildDate><atom:link href="https://nitishpuri.github.io/blog-hugo/tags/deep-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Is there a simple algorithm for intelligence?, Micheal Nelson</title><link>https://nitishpuri.github.io/blog-hugo/books/programming/neuralnets/neural-networks-and-deep-learning-7/</link><pubDate>Tue, 02 Jan 2018 00:00:00 +0000</pubDate><guid>https://nitishpuri.github.io/blog-hugo/books/programming/neuralnets/neural-networks-and-deep-learning-7/</guid><description>Notes for the book.
Source code for the book.
Appendix: Is there a simple algorithm for intelligence?</description></item><item><title>Deep Learning, Micheal Nelson</title><link>https://nitishpuri.github.io/blog-hugo/books/programming/neuralnets/neural-networks-and-deep-learning-6/</link><pubDate>Fri, 29 Dec 2017 00:00:00 +0000</pubDate><guid>https://nitishpuri.github.io/blog-hugo/books/programming/neuralnets/neural-networks-and-deep-learning-6/</guid><description>Notes for the book.
Source code for the book.
Chapter 6: Deep Learning</description></item><item><title>Why are deep neural networks hard to train?, Micheal Nelson</title><link>https://nitishpuri.github.io/blog-hugo/books/programming/neuralnets/neural-networks-and-deep-learning-5/</link><pubDate>Sat, 23 Dec 2017 00:00:00 +0000</pubDate><guid>https://nitishpuri.github.io/blog-hugo/books/programming/neuralnets/neural-networks-and-deep-learning-5/</guid><description>Notes for the book.
Source code for the book.
Chapter 5: Why are deep neural networks hard to train?</description></item><item><title>A visual proof that neural networks can compute any function, Micheal Nelson</title><link>https://nitishpuri.github.io/blog-hugo/books/programming/neuralnets/neural-networks-and-deep-learning-4/</link><pubDate>Thu, 23 Nov 2017 00:00:00 +0000</pubDate><guid>https://nitishpuri.github.io/blog-hugo/books/programming/neuralnets/neural-networks-and-deep-learning-4/</guid><description>Notes for the book.
Source code for the book.
Chapter 4: A visual proof that neural networks can compute any function One of the most striking facts about neural networks is that they can compute any function at all.
That is, suppose someone hands you some complicated, wiggly function, $f(x)$:
No matter what the function, there is guaranteed to be a neural network so that for every possible input, $x$, the value $f(x)$ (or some close approximation) is output from the network.</description></item><item><title>Improving the way neural networks learn, Micheal Nelson</title><link>https://nitishpuri.github.io/blog-hugo/books/programming/neuralnets/neural-networks-and-deep-learning-3/</link><pubDate>Sat, 23 Sep 2017 00:00:00 +0000</pubDate><guid>https://nitishpuri.github.io/blog-hugo/books/programming/neuralnets/neural-networks-and-deep-learning-3/</guid><description>Notes for the book.
Source code for the book.
Chapter 3: Improving the way neural networks learn Backpropagation was the basic swing, the fooundation for learning in most work on neural networks. Now we will learn the tricks.
These include,
A better cost function, known as cross entropy Four regularization methods. A better method for weight initialization. A set of heuristics to help choose good hyper-parameters. And several other techniques, The cross-entropy cost function Yet while unpleasant, we learn quickly when we&amp;rsquo;re decisively wrong.</description></item><item><title>How the backpropogation algorithm works, Micheal Nelson</title><link>https://nitishpuri.github.io/blog-hugo/books/programming/neuralnets/neural-networks-and-deep-learning-2/</link><pubDate>Fri, 22 Sep 2017 00:00:00 +0000</pubDate><guid>https://nitishpuri.github.io/blog-hugo/books/programming/neuralnets/neural-networks-and-deep-learning-2/</guid><description>Notes for the book.
Source code for the book.
Chapter 2: How the backpropogation algorithm works Was introduced in the 70&amp;rsquo;s, but came into light with this paper.
Today, it is the workhorse of learning in neural networks.
Warm up: a fast matrix-based approach to computing the output from a neural network First, the notations,
For weights,
For biases and activations,
These are related,
$$\begin{eqnarray} a^{l}j = \sigma\left( \sum_k w^{l}{jk} a^{l-1}_k + b^l_j \right), \tag{23}\end{eqnarray}$$</description></item><item><title>Using neural nets to recognize handwritten digits, Micheal Nelson</title><link>https://nitishpuri.github.io/blog-hugo/books/programming/neuralnets/neural-networks-and-deep-learning-1/</link><pubDate>Thu, 21 Sep 2017 00:00:00 +0000</pubDate><guid>https://nitishpuri.github.io/blog-hugo/books/programming/neuralnets/neural-networks-and-deep-learning-1/</guid><description>Notes for the book.
Source code for the book.
Chapter 1: Using neural nets to recognize handwritten digits Perceptrons $$\begin{eqnarray} \mbox{output} &amp;amp; = &amp;amp; \left{ \begin{array}{ll} 0 &amp;amp; \mbox{if } \sum_j w_j x_j \leq \mbox{ threshold} \
1 &amp;amp; \mbox{if } \sum_j w_j x_j &amp;gt; \mbox{ threshold} \end{array} \right. \tag{1}\end{eqnarray}$$
Network of perceptrons First, simplify notation, $w \cdot x \equiv \sum_j w_j x_j$
Move, threshold into the network as bias, $b \equiv -\mbox{threshold}$</description></item><item><title>Neural Networks and Deep Learning, Micheal Nelson</title><link>https://nitishpuri.github.io/blog-hugo/books/programming/neuralnets/</link><pubDate>Wed, 20 Sep 2017 00:00:00 +0000</pubDate><guid>https://nitishpuri.github.io/blog-hugo/books/programming/neuralnets/</guid><description>Notes for the book.
Source code for the book.
Chapter 1 : Using neural nets to recognize handwritten digits Chapter 2 : How the backpropogation algorithm works Chapter 3 : Improving the way neural networks learn Chapter 4 : A visual proof that neural networks can compute any function Chapter 5 : Why are deep neural networks hard to train? Chapter 6 : Deep Learning Appendix : Is there a simple algorithm for intelligence?</description></item><item><title>Neural Networks for Computer Vision</title><link>https://nitishpuri.github.io/blog-hugo/research/machine-intelligence/nn-for-cv-1/</link><pubDate>Tue, 05 Sep 2017 00:00:00 +0000</pubDate><guid>https://nitishpuri.github.io/blog-hugo/research/machine-intelligence/nn-for-cv-1/</guid><description>Very minimal notes on some papers or articles that I recently read. Mainly for logging purposes.
Image Recognition Very Deep Convolutional Networks For Large-Scale Image Recognition Karen Simonyan, Andrew Zisserman : Apr 2015
Source
Implementation
Introduces the VGG network that won ImageNet in 2014. Deeper ConvNets. Takes input as (224 X 224) RGB and mean image subtracted as preprocessing. Two final FC hidden layers, followed by one FC layer with 1000 outputs.</description></item><item><title>Research Notes:: Deep Learning</title><link>https://nitishpuri.github.io/blog-hugo/research/machine-intelligence/research-ml/</link><pubDate>Mon, 04 Sep 2017 00:00:00 +0000</pubDate><guid>https://nitishpuri.github.io/blog-hugo/research/machine-intelligence/research-ml/</guid><description>Very minimal notes on some papers or articles that I recently read. Mainly for logging purposes.
Image Recognition and Convnet Architectures Image Recognition Very Deep Convolutional Networks For Large-Scale Image Recognition Going Deeper with Convolutions Deep Residual Learning for Image Recognition Rethinking the Inception Architecture for Computer Vision Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning Xception: Deep Learning with Depthwise Separable Convolutions Deep Visualization Visualizing and Understanding Convolutional Networks Multifaceted Feature Visualization: Uncovering the Different Types of Features Learned By Each Neuron in Deep Neural Networks How transferable are features in deep neural networks?</description></item><item><title>Neural Network Architectures</title><link>https://nitishpuri.github.io/blog-hugo/research/machine-intelligence/nn-arch-1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://nitishpuri.github.io/blog-hugo/research/machine-intelligence/nn-arch-1/</guid><description>Deep Learning Architectures Self-Normalizing Neural Networks GÃ¼nter Klambauer, Thomas Unterthiner, Andreas Mayr, Sepp Hochreiter : Sep 2017
Source
Deep learning is setting new benchmarks everyday with the help of RNNs and CNNs. However, looking at problems that are not related to vision or sequential tasks, gradient boosting, random forests, or support vector machines are winning most of the competitions(Eg. Kaggle, HIGGS Challenge). With CNNs success, batch normalization and other stochastic regularization techniques has evolved into a standard.</description></item><item><title>Object Detection and Image Segmentation</title><link>https://nitishpuri.github.io/blog-hugo/research/machine-intelligence/detect-segment-1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://nitishpuri.github.io/blog-hugo/research/machine-intelligence/detect-segment-1/</guid><description>Image Segmentation Image segmentation review Source
A review of segmentation at qure.ai Rich feature hierarchies for accurate object detection and semantic segmentation Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik : Oct 2014
Source
Introduces R-CNN, Regions with CNN. Bridging the gap between image classification and object detection. Object detection with R-CNN Region proposals. Uses selective search. Propose a bunch of boxes in the image and see if any of them actually correspond to an object.</description></item><item><title>Object Detection and Image Segmentation, Part 2</title><link>https://nitishpuri.github.io/blog-hugo/research/machine-intelligence/detect-segment-2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://nitishpuri.github.io/blog-hugo/research/machine-intelligence/detect-segment-2/</guid><description>Image Segmentation SSD: Single Shot MultiBox Detector Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg : Dec 2016
Source
Code
Accurate approaches for image segmentation and object detection are available, like Faster R-CNN, but they are too slow for real time applications. Eliminates bounding box proposals and the subsequent pixel or feature resampling stage. Improvements include using a small convolutional filter to predict object categories and offsets in bounding box locations, using separate predictors (filters) for different aspect ratio detections, and applying these filters to multiple feature maps from the later stages of a network in order to perform detection at multiple scales.</description></item><item><title>Style Transfer, Part 1</title><link>https://nitishpuri.github.io/blog-hugo/research/machine-intelligence/style-transfer-1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://nitishpuri.github.io/blog-hugo/research/machine-intelligence/style-transfer-1/</guid><description>Style Transfer A Neural Style Algorithm of Artistic Style Leon A. Gatys, Alexander S. Ecker, Matthias Bethge : Sep 2015
Source
In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the content and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. Then we came across Deep Neural Networks.</description></item><item><title>Style Transfer, Part 2</title><link>https://nitishpuri.github.io/blog-hugo/research/machine-intelligence/style-transfer-2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://nitishpuri.github.io/blog-hugo/research/machine-intelligence/style-transfer-2/</guid><description>Style Transfer Artistic style transfer for videos Manuel Ruder, Alexey Dosovitskiy, Thomas Brox : Apr 2016
Source
The previously discussed techniques have been applied to videos on per frame basis. However, processing each frame of the video independently leads to flickering and false discontinuities, since the solution of the style transfer task is not stable. To regularize the transfer temporal constraints using optical flow are introduced. Notation $\mathbf p^{(i)}$ is the $i^{th}$ frame of the original video.</description></item></channel></rss>