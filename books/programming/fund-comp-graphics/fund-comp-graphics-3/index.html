<!doctype html><html lang=en>
<head>
<meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="ie=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta property="og:title" content="Fundamentals of Computer Graphics, Peter Shirley, Part 3">
<meta property="og:description" content="Chapter 8 : The Graphics Pipeline The previous several chapters have established the mathematical scaffolding we need to look at the second major approach to rendering: Drawing objects one by one onto the screen, or object-order rendering. Unlike in ray tracing, where we consider each pixel in turn and find the objects that influence its color, we&rsquo;ll now instead consider each geometric object in turn and find the pixels that it could have an effect on.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://nitishpuri.github.io/blog-hugo/books/programming/fund-comp-graphics/fund-comp-graphics-3/"><meta property="og:image" content="https://nitishpuri.github.io/images/site-feature-image.jpg"><meta property="article:section" content="books">
<meta property="article:modified_time" content="2017-08-17T00:00:00+00:00">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://nitishpuri.github.io/images/site-feature-image.jpg">
<meta name=twitter:title content="Fundamentals of Computer Graphics, Peter Shirley, Part 3">
<meta name=twitter:description content="Chapter 8 : The Graphics Pipeline The previous several chapters have established the mathematical scaffolding we need to look at the second major approach to rendering: Drawing objects one by one onto the screen, or object-order rendering. Unlike in ray tracing, where we consider each pixel in turn and find the objects that influence its color, we&rsquo;ll now instead consider each geometric object in turn and find the pixels that it could have an effect on.">
<link rel=canonical href=https://nitishpuri.github.io/blog-hugo/books/programming/fund-comp-graphics/fund-comp-graphics-3/>
<title>
Fundamentals of Computer Graphics, Peter Shirley, Part 3 | Blog - Nitish Puri
</title>
<link href=https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta1/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-giJF6kkoqNQ00vy+HMDP7azOuL0xtbfIcaT9wjKHr8RbDVddVHyTfAAsrekwKmP1 crossorigin=anonymous>
<link href=../../../../blog-hugo/css/style.css rel=stylesheet>
<script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:['script','noscript','style','textarea','pre']}},window.addEventListener('load',a=>{document.querySelectorAll("mjx-container").forEach(function(a){a.parentElement.classList+='has-jax'})})</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
</head>
<body>
<header class=blog-header>
<nav class="navbar navbar-expand-md navbar-light bg-light">
<div class=container-fluid>
<a class=navbar-brand href=../../../../blog-hugo>
<img src=https://getbootstrap.com/docs/4.1/assets/brand/bootstrap-solid.svg width=30 height=30 class="d-inline-block align-top" alt>
Blog - Nitish Puri
</a>
<button class=navbar-toggler type=button data-bs-toggle=collapse data-bs-target=#navbarNav aria-controls=navbarNav aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span>
</button>
<div class="collapse navbar-collapse justify-content-between" id=navbarNav>
<ul class=navbar-nav>
<li class=nav-item>
<a class=nav-link href=../../../../blog-hugo/blog-hugo/books/></a>
</li>
<li class=nav-item>
<a class=nav-link href=../../../../blog-hugo/blog-hugo/research/></a>
</li>
<li class=nav-item>
<a class=nav-link href=../../../../blog-hugo/blog-hugo/about/>About</a>
</li>
</ul>
</div>
</div>
</nav>
</header>
<div class=container>
<div class=row>
<div class="col-12 col-lg-8 blog-main"><nav class="breadcrumb container" aria-label=breadcrumb>
<ol class=breadcrumb>
<li class=breadcrumb-item> <a href=https://nitishpuri.github.io/blog-hugo/>Home</a> </li>
<li class=breadcrumb-item> <a href=https://nitishpuri.github.io/blog-hugo/books/>Books</a> </li>
<li class=breadcrumb-item> <a href=https://nitishpuri.github.io/blog-hugo/books/programming/>Programming</a> </li>
<li class=breadcrumb-item> <a href=https://nitishpuri.github.io/blog-hugo/books/programming/fund-comp-graphics/>Fundamentals of Computer Graphics, Peter Shirley</a> </li>
<li class="breadcrumb-item active" aria-current=page> Fundamentals of Computer Graphics, Peter Shirley, Part 3</li>
</ol>
</nav>
<header>
<h2 class=blog-post-title>
<a class="text-dark text-decoration-none" href=../../../../blog-hugo/books/programming/fund-comp-graphics/fund-comp-graphics-3/>Fundamentals of Computer Graphics, Peter Shirley, Part 3</a>
</h2>
<div class="blog-post-date text-secondary">
<time datetime=2017-08-17>Aug 17, 2017</time>
by <span rel=author>Nitish Puri</span>
</div>
<div class="blog-post-tags text-secondary">
<strong>Tags:</strong>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/tags/graphics>graphics</a>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/tags/notes>notes</a>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/tags/programming>programming</a>
</div>
<hr>
</header>
<article class=blog-post>
<nav id=TableOfContents>
<ul>
<li><a href=#chapter-8--the-graphics-pipeline>Chapter 8 : The Graphics Pipeline</a>
<ul>
<li><a href=#rasterization>Rasterization</a></li>
<li><a href=#operations-before-and-after-rasterization>Operations Before and After Rasterization</a></li>
<li><a href=#simple-antialiasing>Simple Antialiasing</a></li>
<li><a href=#culling-primitives-for-efficiency>Culling Primitives for Efficiency</a></li>
</ul>
</li>
<li><a href=#chapter-9--signal-processing>Chapter 9 : Signal Processing</a>
<ul>
<li><a href=#digital-audio-sampling-in-1d>Digital Audio: Sampling in 1D</a></li>
<li><a href=#convolution>Convolution</a></li>
<li><a href=#convolution-filters>Convolution Filters</a></li>
<li><a href=#signal-processing-for-images>Signal Processing for Images</a></li>
<li><a href=#sampling-theory>Sampling Theory</a></li>
</ul>
</li>
</ul>
</nav>
<h2 id=chapter-8--the-graphics-pipeline>Chapter 8 : The Graphics Pipeline</h2>
<p>The previous several chapters have established the mathematical scaffolding we need to look at
the second major approach to rendering: Drawing objects one by one onto the screen, or
<em>object-order rendering</em>. Unlike in ray tracing, where we consider each pixel in turn and find the
objects that influence its color, we&rsquo;ll now instead consider each geometric object in turn and find the
pixels that it could have an effect on. The process of finding all the pixels in an image that are occupied
by a geometric primitive is called <em>rasterization</em>, so object-order rendering can also be called rendering
by rasterization. The sequence of operations that is required, starting with objects and ending by updating
pixels in the image, is known as the <em>graphics pipeline</em>.</p>
<p><img src=../../../../images/fundcg/8_pipeline1.png alt=alt></p>
<h3 id=rasterization>Rasterization</h3>
<p>Rasterization is the central operation in object-order graphics, and the <strong>rasterizer</strong> is central to any
graphics pipeline. For each primitive that comes in, the rasterizer has two jobs, it <strong>enumerates</strong> the pixels
that are covered by the primitive and it <strong>interpolates</strong> values, called attributes, across the primitive - the
purpose for these attributes will be clear with later examples. The output of the rasterizer is a set of
<strong>fragments</strong>, one for each pixel covered by the primitive. Each fragment <em>lives</em> a a particular pixel and
carries uts own set of attribute values.</p>
<h4 id=line-drawing>Line Drawing</h4>
<p><strong>Line Drawing Using Implicit Line Equations</strong><br>
$$f(x, y) \equiv (y_0-y_1)x+(x_1-x_0)y+x_0y_1-x_1y_0 = 0$$</p>
<p><img src=../../../../images/fundcg/8_line1.png alt=alt> <img src=../../../../images/fundcg/8_line2.png alt=alt></p>
<h4 id=triangle-rasterization>Triangle Rasterization</h4>
<p><img src=../../../../images/fundcg/8_triangle1.png alt=alt></p>
<h4 id=clipping>Clipping</h4>
<p><img src=../../../../images/fundcg/8_clipping.png alt=alt></p>
<p>The two most common approaches for implementing clipping are</p>
<ol>
<li>in world coordinates using the six planes that bound the truncated viewing pyramid,</li>
<li>in the 4D transformed space before the homogeneous divide.</li>
</ol>
<p>This is the basic approach,.</p>
<p>$$ \mathbf{for};each;of;six;planes;\mathbf{do} \\<br>
\quad \mathbf{if};(triangle;entirely;outside;of;plane);\mathbf{then} \\\<br>
\quad \quad break (triangle;is;not;visible)\\<br>
\quad \mathbf{else;if};triangle;spans;plane;\mathbf{then}\\<br>
\quad \quad clip;triangle\<br>
\quad \mathbf{if};(quadrilateral;is;left);\mathbf{then}\<br>
\quad \quad break;into;two;triangles\<br>
$$</p>
<p>Clipping before the transform(Option 1)<br>
A straightforward implementation. Only question is, what are the six planes,.?</p>
<p>These can be inferred from the vector geometry directly or can be computed by applying inverse transform on the eight vertices of the view volume.</p>
<p>Clipping in Homogeneous Coordinates (Option 2)<br>
Surprisingly, the option usually implemented is that of clipping in homogeneous coordinates before the divide. Here the view volume is 4D, and it is bounded by 3D volumes (hyperplanes).</p>
<h3 id=operations-before-and-after-rasterization>Operations Before and After Rasterization</h3>
<p>Before a primitive can be rasterized, the vertices that define it must be in screen coordinates, and the colors
or other attributes that are supposed to be interpolated across the primitive must be known. Preparing this data
is the job of the <strong>vertex processing</strong> stage of the pipeline. In this stage, incoming vertices are transformed by the modeling, viewing, and projection transformations, mapping them from their original
coordinates into screen space (where, recall, position is measured in terms of pixels). At the same time, other information, such as colors, surface normals, or texture coordinates, is transformed as needed; we’ll discuss these additional attributes in the examples below.</p>
<p>After rasterization, further processing is done to compute a color and depth for each fragment. This processing
can be as simple as just passing through an interpolated color and using the depth computed by the rasterizer;
or it can involve complex shading operations. Finally, the blending phase combines the fragments generated by
the (possibly several) primitives that overlapped each pixel to compute the final color. The most common
blending approach is to choose the color of the fragment with the smallest depth (closest to the eye).</p>
<h4 id=simple-2d-drawing><strong>Simple 2D Drawing</strong></h4>
<p>The simplest possible pipeline does nothing in the vertex or fragment stages, and in the blending stage the
color of each fragment simply overwrites the value of the previous one. The application supplies primitives
directly in pixel coordinates, and the rasterizer does all the work. This basic arrangement is the essence of
many simple, older APIs for drawing user interfaces, plots, graphs, and other 2D content. Solid color shapes can
be drawn by specifying the same color for all vertices of each primitive, and our model pipeline also supports
smoothly varying color using interpolation.</p>
<h4 id=a-minimal-3d-pipeline><strong>A Minimal 3D Pipeline</strong></h4>
<p>To draw objects in 3D, the only change needed to the 2D drawing pipeline is a single matrix transformation: the
vertex-processing stage multiplies the incoming vertex positions by the product of the modeling, camera,
projection, and viewport matrices, resulting in screen-space triangles that are then drawn in the same way as if
they’d been specified directly in 2D.<br>
One problem with the minimal 3D pipeline is that in order to get occlusion relationships correct—to get nearer
objects in front of farther away objects— primitives must be drawn in back-to-front order. This is known as the
<strong>painter’s algorithm</strong> for hidden surface removal, by analogy to painting the background of a painting first,
then painting the foreground over it.</p>
<p><img src=../../../../images/fundcg/8_raster1.png alt=alt> <img src=../../../../images/fundcg/8_raster2.png alt=alt></p>
<h4 id=using-a-z-buffer-for-hidden-surface-removal><strong>Using a Z-Buffer for Hidden surface removal</strong></h4>
<p>The z-buffer algorithm is implemented in the fragment blending phase, by comparing the depth of each fragment
with the current value stored in the z-buffer. If the fragment’s depth is closer, both its color and its depth
value overwrite the values currently in the color and depth buffers. If the fragment’s depth is farther away, it
is discarded. To ensure that the first fragment will pass the depth test, the z buffer is initialized to the
maximum depth (the depth of the far plane). Irrespective of the order in which surfaces are drawn, the same
fragment will win the depth test, and the image will be the same.<br>
<img src=../../../../images/fundcg/8_zbuffer1.png alt=alt></p>
<h4 id=per-vertex-shading><strong>Per vertex Shading</strong></h4>
<p>One way to handle shading computations is to perform them in the vertex stage. The application provides normal vectors at the vertices, and the positions and colors of the lights are provided separately (they don’t vary across the surface, so they don’t need to be specified for each vertex). For each vertex, the direction to the viewer and the direction to each light are computed based on the positions of the camera, the lights, and the vertex. The desired shading equation is evaluated to compute a color, which is then passed to the rasterizer as the vertex color. Pervertex shading is sometimes called <strong>Gouraud shading</strong>.</p>
<p>Per-vertex shading has the disadvantage that it cannot produce any details in the shading that are smaller than the primitives used to draw the surface, because it only computes shading once for each vertex and never in between vertices. For instance, in a room with a floor that is drawn using two large triangles and illuminated by a light source in the middle of the room, shading will be evaluated only at the corners of the room, and the interpolated value will likely be much too dark in the center. Also, curved surfaces that are shaded with specular highlights must be drawn using primitives small enough that the highlights can be resolved.</p>
<h4 id=per-fragment-shading><strong>Per-fragment Shading</strong></h4>
<p>To avoid the interpolation artifacts associated with per-vertex shading, we can avoid interpolating colors by performing the shading computations <em>after</em> the interpolation, in the fragment stage. In per-fragment shading, the same shading equations are evaluated, but they are evaluated for each fragment using interpolated vectors, rather than for each vertex using the vectors from the application.</p>
<h4 id=texture-mapping><strong>Texture Mapping</strong></h4>
<p><strong>Textures</strong> (discussed in Chapter 11) are images that are used to add extra detail to the shading of surfaces that would otherwise look too homogeneous and artificial. The idea is simple: each time shading is computed, we read one of the values used in the shading computation—the diffuse color, for instance—from a texture instead of using the attribute values that are attached to the geometry being rendered. This operation is known as a texture lookup: the shading code specifies a <strong>texture coordinate</strong>, a point in the domain of the texture, and the texture-mapping system finds the value at that point in the texture image and returns it. The texture
value is then used in the shading computation.<br>
The most common way to define texture coordinates is simply to make the texture coordinate another vertex attribute. Each primitive then knows where it lives in the texture.</p>
<h3 id=simple-antialiasing>Simple Antialiasing</h3>
<p>Just as with ray tracing, rasterization will produce jagged lines and triangle edges if we make an all-or-nothing determination of whether each pixel is inside the primitive or not. In fact, the set of fragments generated by the simple triangle rasterization algorithms described in this chapter, sometimes called standard or <strong>aliased</strong> rasterization, is exactly the same as the set of pixels that would be mapped to that triangle by a ray tracer that sends one ray through the center of each pixel. Also as in ray tracing, the solution is to allow pixels to be partly covered by a primitive (Crow, 1978). In practice this form of blurring helps visual quality, especially in animations. This is shown as the top line of Figure 8.15.<br>
<img src=../../../../images/fundcg/8_aliasing.png alt=alt></p>
<p>The easiest way to implement box-filter antialiasing is by <strong>supersampling</strong>: create images at very high resolutions and then downsample. For example, if our goal is a 256 × 256 pixel image of a line with width 1.2 pixels, we could rasterize a rectangle version of the line with width 4.8 pixels on a 1024 × 1024 screen, and then average 4 × 4 groups of pixels to get the colors for each of the 256 × 256 pixels in the “shrunken” image. This is an approximation of the actual boxfiltered image, but works well when objects are not extremely small relative to the distance between pixels.<br>
Supersampling is quite expensive, however. Because the very sharp edges that cause aliasing are normally caused by the edges of primitives, rather than sudden variations in shading within a primitive, a widely used optimization is to sample visibility at a higher rate than shading. If information about coverage and depth is stored for several points within each pixel, very good antialiasing can be achieved even if only one color is computed. In systems like RenderMan that use per-vertex shading, this is achieved by rasterizing at high resolution: it is inexpensive to do so because shading is simply interpolated to produce colors for the many fragments, or visibility samples. In systems with per-fragment shading, such as hardware pipelines, <strong>multisample antialiasing</strong> is achieved by storing for each fragment a single color plus a coverage mask and a set of depth values.</p>
<h3 id=culling-primitives-for-efficiency>Culling Primitives for Efficiency</h3>
<p>The strength of object-order rendering, that it requires a single pass over all the geometry in the scene, is also a weakness for complex scenes. For instance, in a model of an entire city, only a few buildings are likely to be visible at any given time. A correct image can be obtained by drawing all the primitives in the scene, but a great deal of effort will be wasted processing geometry that is behind the visible buildings, or behind the viewer, and therefore doesn’t contribute to the final image.</p>
<p>Identifying and throwing away invisible geometry to save the time that would be spent processing it is known as <strong>culling</strong>.</p>
<ul>
<li><strong>view volume culling</strong> : the removal of geometry that is outside the view volume;</li>
<li><strong>occlusion culling</strong> : the removal of geometry that may be within the view volume but is obscured, or occluded, by other geometry closer to the camera;</li>
<li><strong>backface culling</strong> : the removal of primitives facing away from the camera.</li>
</ul>
<h4 id=view-volume-culling><strong>View Volume Culling</strong></h4>
<p>When an entire primitive lies outside the view volume, it can be culled, since it will produce no fragments when rasterized. If we can cull many primitives with a quick test, we may be able to speed up drawing significantly. On the other hand, testing primitives individually to decide exactly which ones need to be drawn may cost more than just letting the rasterizer eliminate them.<br>
View volume culling, also known as <strong>view frustum culling</strong>, is especially helpful when many triangles are grouped into an object with an associated bounding volume. If the bounding volume lies outside the view volume, then so do all the triangles that make up the object. For example, if we have 1000 triangles bounded by a single sphere with center $\mathbf{c}$ and radius $\mathbf{r}$, we can check whether the sphere lies outside the clipping plane,</p>
<h4 id=backface-culling><strong>Backface Culling</strong></h4>
<p>When polygonal models are closed, i.e., they bound a closed space with no holes, then they are often assumed to have outward facing normal vectors as discussed in Chapter 10. For such models, the polygons that face away from the eye are certain to be overdrawn by polygons that face the eye. Thus, those polygons can be culled before the pipeline even starts.</p>
<h2 id=chapter-9--signal-processing>Chapter 9 : Signal Processing</h2>
<h3 id=digital-audio-sampling-in-1d>Digital Audio: Sampling in 1D</h3>
<p><img src=../../../../images/fundcg/9_audio1.png alt=alt><br>
<img src=../../../../images/fundcg/9_audio2.png alt=alt></p>
<h4 id=sampling-artifacts-and-aliasing><strong>Sampling Artifacts and aliasing</strong></h4>
<p>The digital audio recording chain can serve as a concrete model for the sampling and reconstruction processes that happen in graphics. The same kind of undersampling and reconstruction artifacts also happen with images or other sampled signals in graphics, and the solution is the same: filtering before sampling and filtering again during reconstruction.</p>
<p>Once the sampling has been done, it is impossible to know which of the two signals—the fast or the slow sine wave—was the original, and therefore there is no single method that can properly reconstruct the signal in both cases. Because the high frequency signal is “pretending to be” a low-frequency signal, this phenomenon is known as <strong>aliasing</strong>.</p>
<p>Questions,..</p>
<ul>
<li>What sample rate is high enough to ensure good results?</li>
<li>What kinds of filters are appropriate for sampling and reconstruction?</li>
<li>What degree of smoothing is required to avoid aliasing?</li>
</ul>
<h3 id=convolution>Convolution</h3>
<p>Convolution is an operation on functions: it takes two functions and combines them to produce a new function. In this book, the convolution operator is denoted by a star: the result of applying convolution to the functions $f$ and $g$ is $f \star g$. We say that $f$ is convolved with $g$, and $f \star g$ is the convolution of $f$ and $g$.</p>
<h4 id=moving-averages><strong>Moving Averages</strong></h4>
<p><img src=../../../../images/fundcg/9_conv1.png alt=alt></p>
<p>$$h(x) = \frac{1}{2r}\int_{x-r}^{x+r}g(t)dt \<br>
c[i] = \frac{1}{2r+1}\sum_{j=i-r}^{i+r}b[j]$$</p>
<h4 id=discrete-convolution><strong>Discrete Convolution</strong></h4>
<p><img src=../../../../images/fundcg/9_conv2.png alt=alt></p>
<p>$$(a \star b)[i] = \sum_{j=-r}^{r}a[j]b[i-j]$$</p>
<p>$$\mathbf{function}; convolve(sequence;a,;sequence;b,;int;r,;int;i)\<br>
\quad s = 0\<br>
\quad \mathbf{for};;j = -r;\text{to};r\<br>
\quad \quad s = s+a[j]b[i-j]\<br>
\quad \mathbf{return};;s $$</p>
<h4 id=convulution-filters><strong>Convulution Filters</strong></h4>
<p>Box filter,<br>
$$a[j] = \begin{cases}
\frac{1}{2r+1} & -r \leq j \leq r\<br>
0 & \text{otherwise.}\end{cases}$$</p>
<p><img src=../../../../images/fundcg/9_conv3.png alt=alt><br>
<img src=../../../../images/fundcg/9_conv4.png alt=alt></p>
<h4 id=properties-of-convolution><strong>Properties of Convolution</strong></h4>
<ul>
<li><strong>commutative:</strong> $(a b)[i] = (b a)[i]$</li>
<li><strong>associative:</strong> $(a (b c))[i] = ((a b) c)[i]$</li>
<li><strong>distributive:</strong> $(a (b + c))[i] = (a b + a c)[i]$</li>
</ul>
<h4 id=convolution-as-a-sum-of-shifted-filters><strong>Convolution as a Sum of Shifted Filters</strong></h4>
<p><img src=../../../../images/fundcg/9_conv5.png alt=alt></p>
<h4 id=convolution-with-continuous-functions><strong>Convolution with Continuous Functions</strong></h4>
<p>$$(f\star g)(x) = \int_{-\inf}^{+\inf}f(t)g(x-t)dt$$</p>
<p><img src=../../../../images/fundcg/9_conv6.png alt=alt></p>
<p><em>Example (Convolution of two box functions).</em></p>
<p><img src=../../../../images/fundcg/9_conv7.png alt=alt></p>
<h4 id=the-dirac-delta-function><strong>The Dirac Delta Function</strong></h4>
<p>In discrete convolution, we saw that the discrete impulse d acted as an identity: d a = a. In the continuous case, there is also an identity function, called the <em>Dirac impulse</em> or <em>Dirac delta function</em>, denoted
$\delta(x)$.<br>
<img src=../../../../images/fundcg/9_dirac1.png alt=alt></p>
<p>$$\int_{-\inf}^{\inf}\delta(x)f(x)dx = f(0).$$<br>
<img src=../../../../images/fundcg/9_dirac2.png alt=alt></p>
<h4 id=discrete-continuous-convolution><strong>Discrete-Continuous Convolution</strong></h4>
<p><img src=../../../../images/fundcg/9_conv8.png alt=alt></p>
<p>$$\mathbf{function}\quad reconstruct,(sequence; a, filter; f, real; x)\<br>
s = 0\<br>
r = f.radius\<br>
\mathbf{for} \quad i = x - r \text{ to } x+r;\mathbf{do}\<br>
\quad s = s + a[i]f(x-i)\<br>
\mathbf{return}\quad s$$</p>
<h4 id=convolution-in-more-than-one-dimension><strong>Convolution in More Than One Dimension</strong></h4>
<p>$$(a \star b)[i, j] = \sum_{i'}\sum_{j'}a[i', j&rsquo;b[i - i', j - j']$$</p>
<p><img src=../../../../images/fundcg/9_conv9.png alt=alt></p>
<p>$$\mathbf{function}\quad convolve2d,(\text{sequence2d};a, \text{sequence2d};b,\text{int};i,\text{int};j)\<br>
s = 0\ r = a.radius\<br>
\mathbf{for}\quad i = -r \text{ to } r; \mathbf{do}\<br>
\quad \mathbf{for}\quad j = -r \text{ to } r;\mathbf{do}\<br>
\quad \quad s = s+a[i][j]b[i-i'][j-j']\<br>
\mathbf{return }; s$$</p>
<p>$$\begin{align}(f \star g)(x, y) & = & \int \int f(x', y')g(x - x', y - y')dx&rsquo;dy';\<br>
(a \star f)(x, y) & = & \sum_i \sum_j a[i, j]f(x-i, y-j).\end{align}$$</p>
<h3 id=convolution-filters>Convolution Filters</h3>
<h4 id=a-gallery-of-convolution-filters><strong>A Gallery of Convolution Filters</strong></h4>
<p><strong>The Box Filter</strong><br>
<img src=../../../../images/fundcg/9_convf1.png alt=alt></p>
<p>$$\begin{align}
a_{box,r}[i] & = & \begin{cases}1/(2r_1) & |i| \leq r, \
0 & \text{otherwise.}\end{cases} \<br>
f_{box, r}(x) &= & \begin{cases}1/2r & -r \leq x \leq r, \
0 & \text{otherwise.}\end{cases}
\end{align}$$</p>
<p><strong>The Tent Filter.</strong><br>
<img src=../../../../images/fundcg/9_convf2.png alt=alt></p>
<p>$$\begin{align}
f_{tent}(x) & = & \begin{cases}1 - |x| & |x| \leq 1, \
0 & \text{otherwise.}\end{cases} \<br>
f_{tent, r}(x) &= & \frac{f_{tent}(x/r)}r.
\end{align}$$</p>
<p><strong>The Gaussian Filter</strong>.<br>
Also known as the normal distribution is an important filer theoretically and practically.<br>
<img src=../../../../images/fundcg/9_convf3.png alt=alt></p>
<p>$$f_g(x) = \frac{1}{\sqrt{2\pi}}e^{-x^2/2}.$$</p>
<p><strong>The B-spline cubic filter</strong>.<br>
The blending function for spline curves. It has continuos first and second derivatives &ndash; that is, it is $C_2$
A more concise way of defining this filter is $F_B = f_{box} X f_{box} X f_{box} X f_{box}$.<br>
<img src=../../../../images/fundcg/9_convf4.png alt=alt></p>
<p>$$\begin{align}f_B(x) = \frac16 \begin{cases}
-3(1-|x|)^3 + 3(1 - |x|)^2 + 3(1 - |x|) + 1 & -1 \leq x \leq 1.\<br>
(2 - |x|)^3 & 1 \leq |x| \leq 2.\<br>
0 & \text{otherwise}.\end{cases}\end{align}$$</p>
<p><strong>The Catmull-RomCubic Fiilter.</strong><br>
<img src=../../../../images/fundcg/9_convf5.png alt=alt></p>
<p>$$\begin{align}f_C(x) = \frac12 \begin{cases}
-3(1-|x|)^3 + 4(1 - |x|)^2 + (1 - |x|) & -1 \leq x \leq 1.\<br>
(2 - |x|)^3 - (2-|x|)^2& 1 \leq |x| \leq 2.\<br>
0 & \text{otherwise}.\end{cases}\end{align}$$</p>
<p><strong>The Mitchell-Netravali Cubic Filter.</strong><br>
<img src=../../../../images/fundcg/9_convf6.png alt=alt></p>
<p>$$\begin{align}f_M(x) &= \frac13f_B(x) + \frac23f_C(x) \<br>
&= \frac1{18} \begin{cases}
-21(1-|x|)^3 + 27(1 - |x|)^2 + 9(1 - |x|) + 1 & -1 \leq x \leq 1.\<br>
7(2 - |x|)^3 - 6(2-|x|)^2& 1 \leq |x| \leq 2.\<br>
0 & \text{otherwise}.\end{cases}\end{align}$$</p>
<h4 id=properties-of-filters><strong>Properties of Filters</strong></h4>
<p><img src=../../../../images/fundcg/9_filter1.png alt=alt>
<img src=../../../../images/fundcg/9_filter2.png alt=alt></p>
<p>A continuous filter has a <strong>degree of continuity</strong>, which is the highest-order derivative that is defined everywhere<br>
<img src=../../../../images/fundcg/9_filter3.png alt=alt></p>
<h4 id=separable-filters><strong>Separable Filters</strong></h4>
<p>$$\begin{align}f_2(x, y) = f_1(x)f_1(y)\end{align}$$</p>
<p>Example,</p>
<p>$$\begin{align}f_{2, tent}(x, y) = \begin{cases}
(1 - |x|)(1-|y|) & |x| &lt; 1 ; & ; |y| &lt; 1, \<br>
0 & \text{otherwise.}\end{cases}\end{align}$$<br>
<img src=../../../../images/fundcg/9_filter4.png alt=alt></p>
<p>$$\begin{align}f_{2, g}(x, y) &= \frac1{2\pi}(e^{-x^2/2}e^{-y^2/2}),\<br>
&=\frac1{2\pi}(e^{-(x^2+y^2)/2}),\<br>
&=\frac1{2\pi}(e^{-r^2/2}).\end{align}$$<br>
<img src=../../../../images/fundcg/9_filter5.png alt=alt></p>
<p>Notice that this is (up to a scale factor) the same function we would get if we revolved the 1D Gaussian around the origin to produce a circularly symmetric function. The property of being both circularly symmetric and separable at the same time is unique to the Gaussian function. The profiles along the coordinate axes are Gaussians, but so are the profiles along any direction at any offset from the center.</p>
<p><img src=../../../../images/fundcg/9_filter6.png alt=alt></p>
<p>This savings has great significance for large filters. Filtering an $m \times n$ 2D image with a filter of radius $r$ in the general case requires computation of $(2r+1)^2$ products per pixel, while filtering the image with a separable filter of the same size requires $2(2r + 1)$ products (at the expense of some intermediate storage). This change in asymptotic complexity from $O(r^2)$ to $O(r)$ enables the use of much larger filters.</p>
<p>The algorithm is,</p>
<p>$$\mathbf{function}\quad filterImage(\text{image} I, \text{filter} f)\<br>
r = f.radius \ x = I.width \ y = I.height \<br>
\text{allocate storage array } S[0, &mldr; , n_{x-1}]\<br>
\text{allocate image } I_{out}[r,&mldr; n_x - r - 1][r,&mldr; n_y - r - 1] \<br>
\text{initialize } S \text{ and } I_{out} \text{ to all zeroes} \<br>
\mathbf{for} \quad y = r \text{ to } n_y - r - 1 ; \mathbf{do} \<br>
\quad \mathbf{for} \quad x = 0 \text{ to } n_x - 1 ; \mathbf{do} \<br>
\quad \quad S[x] = 0 \<br>
\quad \quad \mathbf{for}\quad i = -r \text{ to } r ; \mathbf{do} \<br>
\quad \quad \quad S[x] = S[x] + f[i]I[x][y-i] \<br>
\quad \mathbf{for}\quad x = r \text{ to } n_x - r-1 ; \mathbf{do}\<br>
\quad \quad \mathbf{for}\quad i = -r \text{ to } r ; \mathbf{do} \<br>
\quad \quad \quad I_{out}[x][y] = I_{out}[x][y] + f[i]S[x-i] \<br>
\mathbf{return} I_{out}$$</p>
<h3 id=signal-processing-for-images>Signal Processing for Images</h3>
<p><strong>Image Filtering Using Discrete Filters</strong></p>
<p><img src=../../../../images/fundcg/9_filter7.jpg alt=alt> <br>
<img src=../../../../images/fundcg/9_filter8.jpg alt=alt></p>
<p>$$\begin{align}I_{sharp} &= (1+\alpha)I - \alpha(f_{g, \sigma}\star I)\<br>
&=((1+\alpha)d - \alpha f_{g, \sigma})\star I\<br>
&=f_{sharp}(\sigma, \alpha) \star I\end{align}$$</p>
<p><strong>Antialiasing in Image Sampling</strong><br>
<img src=../../../../images/fundcg/9_aliasing1.jpg alt=alt></p>
<p><strong>Reconstruction and Resampling</strong><br>
<img src=../../../../images/fundcg/9_aliasing2.png alt=alt> <br>
<img src=../../../../images/fundcg/9_aliasing3.png alt=alt></p>
<p>$$\mathbf{function}\quad resample(\text{sequence};a, \text{float};x_0, \text{float};\Delta x,
\text{int};n, \text{filter};f) \<br>
\text{create sequence } b \text{ of length } n \<br>
\mathbf{for}\quad i = 0 \text{ to }; n-1; \mathbf{do} \<br>
\quad b[i] = reconstruct(a, f, x_0, + i\Delta x) \<br>
\mathbf{return} ; b\ $$</p>
<p><img src=../../../../images/fundcg/9_aliasing4.png alt=alt></p>
<p>$$\mathbf{function}\quad resample(\text{sequence};a, \text{float};x_l,\text{float};x_h,
\text{int};n, \text{filter};f) \<br>
\text{create sequence } b \text{ of length } n \<br>
r = f.radius \<br>
x_0 = x_l + \Delta x/ 2 \<br>
\mathbf{for}\quad i = 0 \text{ to }; n-1; \mathbf{do} \<br>
\quad s = 0 \ \quad x = x_0 + i \Delta x \<br>
\quad \mathbf{for}\quad j = x - r \text{ to }; x+r; \mathbf{do} \<br>
\quad \quad s = s + a[j]f(x-j) \<br>
\quad b[i] = s
\mathbf{return} ; b\ $$</p>
<p><img src=../../../../images/fundcg/9_aliasing5.jpg alt=alt></p>
<h3 id=sampling-theory>Sampling Theory</h3>
<p><img src=../../../../images/fundcg/9_sample1.png alt=alt></p>
<p>$$\begin{align}\sum_{n = 1, 3, 5, &mldr;}^ \inf \frac4{\pi n} sin,2\pi nx\end{align}$$</p>
<p><img src=../../../../images/fundcg/9_sample2.png alt=alt></p>
<p>$$\begin{align}\frac{sin,\pi u}{\pi u}cos,2\pi ux,du\end{align}$$</p>
<p>When a function $f$ is expressed in this way, this weight, which is a function of the frequency $u$, is called the <strong>Fourier transform</strong> of $f$, denoted $\hat f$.</p>
<p><img src=../../../../images/fundcg/9_sample3.png alt=alt> <br>
<img src=../../../../images/fundcg/9_sample4.png alt=alt></p>
<h4 id=convolution-and-the-fourier-transform><strong>Convolution and the Fourier Transform</strong></h4>
<p><img src=../../../../images/fundcg/9_sample5.png alt=alt></p>
<p>$$\hat f \star \hat g = \mathcal{F}{fg}.$$</p>
<h4 id=a-gallery-of-fourier-transforms><strong>A Gallery of Fourier Transforms</strong></h4>
<p><img src=../../../../images/fundcg/9_sample6.png alt=alt></p>
<h4 id=sampling-and-aliasing><strong>Sampling and Aliasing</strong></h4>
<p><img src=../../../../images/fundcg/9_sample7.jpg alt=alt></p>
<p><strong>Preventing Aliasing in Sampling</strong><br>
<img src=../../../../images/fundcg/9_sample8.jpg alt=alt> <br>
<img src=../../../../images/fundcg/9_sample9.png alt=alt> <br>
<img src=../../../../images/fundcg/9_sample10.jpg alt=alt> <br>
<img src=../../../../images/fundcg/9_sample11.jpg alt=alt> <br>
<img src=../../../../images/fundcg/9_sample12.png alt=alt></p>
<footer>
<h4>See also</h4>
<ul>
<li><a href=../../../../blog-hugo/books/programming/fund-comp-graphics/fund-comp-graphics-1/>Fundamentals of Computer Graphics, Peter Shirley, Part 1</a></li>
<li><a href=../../../../blog-hugo/books/programming/fund-comp-graphics/fund-comp-graphics-2/>Fundamentals of Computer Graphics, Peter Shirley, Part 2</a></li>
<li><a href=../../../../blog-hugo/books/programming/fund-comp-graphics/fund-comp-graphics-4/>Fundamentals of Computer Graphics, Peter Shirley, Part 4</a></li>
<li><a href=../../../../blog-hugo/books/programming/fund-comp-graphics/fund-comp-graphics-5/>Fundamentals of Computer Graphics, Peter Shirley, Part 5</a></li>
<li><a href=../../../../blog-hugo/books/programming/fund-comp-graphics/fund-comp-graphics-6/>Fundamentals of Computer Graphics, Peter Shirley, Part 6</a></li>
</ul>
</footer>
</article>
</div>
<aside class="col-12 col-lg-3 ml-auto blog-sidebar">
<section>
<h4>Recent Posts</h4>
<ol class=list-unstyled>
<li>
<a href=../../../../blog-hugo/books/programming/the-little-schemer/>The Little Schemer</a>
</li>
<li>
<a href=../../../../blog-hugo/books/programming/game-engine-architecture/>Game Engine Architecture</a>
</li>
<li>
<a href=../../../../blog-hugo/books/programming/neuralnets/neural-networks-and-deep-learning-7/>Is there a simple algorithm for intelligence?, Micheal Nelson</a>
</li>
<li>
<a href=../../../../blog-hugo/books/programming/neuralnets/neural-networks-and-deep-learning-6/>Deep Learning, Micheal Nelson</a>
</li>
<li>
<a href=../../../../blog-hugo/books/programming/neuralnets/neural-networks-and-deep-learning-5/>Why are deep neural networks hard to train?, Micheal Nelson</a>
</li>
</ol>
</section>
<section>
<h4>Categories</h4>
<p>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/categories/books>books</a>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/categories/courses>courses</a>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/categories/machine-intelligence>machine-intelligence</a>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/categories/notes>notes</a>
</p>
<h4>Tags</h4>
<p>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/tags/algorithms>algorithms</a>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/tags/architecture>architecture</a>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/tags/biorobots>biorobots</a>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/tags/book>book</a>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/tags/data-science>data-science</a>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/tags/deep-learning>deep-learning</a>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/tags/design>design</a>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/tags/game-engine>game-engine</a>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/tags/graphics>graphics</a>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/tags/image-segmentation>image-segmentation</a>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/tags/kuka>kuka</a>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/tags/lisp>lisp</a>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/tags/machine-intelligence>machine-intelligence</a>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/tags/notes>notes</a>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/tags/opengl>opengl</a>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/tags/philosophy>philosophy</a>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/tags/programming>programming</a>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/tags/projects>projects</a>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/tags/robotics>robotics</a>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/tags/scheme>scheme</a>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/tags/style-transfer>style-transfer</a>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/tags/udacity>udacity</a>
</p>
</section>
</aside>
</div>
</div>
<script src=https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta1/dist/js/bootstrap.bundle.min.js integrity=sha384-ygbV9kiqUc6oa4msXn9868pTtWMgiQaeYH7/t7LECLbyPA2x65Kgf80OJFdroafW crossorigin=anonymous></script>
</body>
</html>