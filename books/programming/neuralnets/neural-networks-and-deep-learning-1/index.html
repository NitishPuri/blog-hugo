<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
<meta http-equiv="x-ua-compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<meta property="og:title" content="Using neural nets to recognize handwritten digits, Micheal Nelson" />
<meta property="og:description" content="Notes for the book.
Source code for the book.
Chapter 1: Using neural nets to recognize handwritten digits Perceptrons $$\begin{eqnarray} \mbox{output} &amp; = &amp; \left{ \begin{array}{ll} 0 &amp; \mbox{if } \sum_j w_j x_j \leq \mbox{ threshold} \
1 &amp; \mbox{if } \sum_j w_j x_j &gt; \mbox{ threshold} \end{array} \right. \tag{1}\end{eqnarray}$$
Network of perceptrons First, simplify notation, $w \cdot x \equiv \sum_j w_j x_j$
Move, threshold into the network as bias, $b \equiv -\mbox{threshold}$" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.myproductionurl.com/books/programming/neuralnets/neural-networks-and-deep-learning-1/" /><meta property="og:image" content="https://www.myproductionurl.com/images/site-feature-image.jpg"/><meta property="article:section" content="books" />

<meta property="article:modified_time" content="2017-09-21T00:00:00+00:00" />


<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://www.myproductionurl.com/images/site-feature-image.jpg"/>

<meta name="twitter:title" content="Using neural nets to recognize handwritten digits, Micheal Nelson"/>
<meta name="twitter:description" content="Notes for the book.
Source code for the book.
Chapter 1: Using neural nets to recognize handwritten digits Perceptrons $$\begin{eqnarray} \mbox{output} &amp; = &amp; \left{ \begin{array}{ll} 0 &amp; \mbox{if } \sum_j w_j x_j \leq \mbox{ threshold} \
1 &amp; \mbox{if } \sum_j w_j x_j &gt; \mbox{ threshold} \end{array} \right. \tag{1}\end{eqnarray}$$
Network of perceptrons First, simplify notation, $w \cdot x \equiv \sum_j w_j x_j$
Move, threshold into the network as bias, $b \equiv -\mbox{threshold}$"/>



    <link rel="canonical" href="https://www.myproductionurl.com/books/programming/neuralnets/neural-networks-and-deep-learning-1/">

    <title>
      
        Using neural nets to recognize handwritten digits, Micheal Nelson | Blog - Nitish Puri
      
    </title>

    
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-giJF6kkoqNQ00vy+HMDP7azOuL0xtbfIcaT9wjKHr8RbDVddVHyTfAAsrekwKmP1" crossorigin="anonymous">
    <link href='../../../../css/style.css' rel="stylesheet">

    

        
    

    

  </head>
  <body>
    
      

<header class="blog-header">
    <nav class="navbar navbar-expand-md navbar-light bg-light">
        <div class="container-fluid">
            <a class="navbar-brand" href="../../../../">
                <img src="https://getbootstrap.com/docs/4.1/assets/brand/bootstrap-solid.svg" width="30" height="30" class="d-inline-block align-top"
    alt="">
Blog - Nitish Puri
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false"
                aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse justify-content-between" id="navbarNav">
                <ul class="navbar-nav">
                    
                    
                    <li class="nav-item">
                        
                            
                            <a class="nav-link " href="../../../../books/"></a>
                        
                    </li>
                    
                    <li class="nav-item">
                        
                            
                            <a class="nav-link " href="../../../../research/"></a>
                        
                    </li>
                    
                    <li class="nav-item">
                        
                            
                            <a class="nav-link " href="../../../../about/">About</a>
                        
                    </li>
                    
                </ul>
                
            </div>
        </div>
    </nav>
</header>

    

    
    <div class="container">
      <div class="row">
        <div class="col-12 col-lg-8 blog-main"><nav class="breadcrumb container" aria-label="breadcrumb"  >
    <ol class="breadcrumb">
    
    
    
        
    
        
    
        
    
        
    
        
    
    
        <li class="breadcrumb-item"> <a href="https://www.myproductionurl.com/">Home</a> </li>
    
    
        <li class="breadcrumb-item"> <a href="https://www.myproductionurl.com/books/">Books</a> </li>
    
    
        <li class="breadcrumb-item"> <a href="https://www.myproductionurl.com/books/programming/">Programming</a> </li>
    
    
        <li class="breadcrumb-item"> <a href="https://www.myproductionurl.com/books/programming/neuralnets/">Neural Networks and Deep Learning, Micheal Nelson</a> </li>
    
    
    <li class="breadcrumb-item active" aria-current="page">  Using neural nets to recognize handwritten digits, Micheal Nelson</li>
    
    
</ol>
</nav>

<header>
    <h2 class="blog-post-title">
    <a class="text-dark text-decoration-none" href="../../../../books/programming/neuralnets/neural-networks-and-deep-learning-1/">Using neural nets to recognize handwritten digits, Micheal Nelson</a>
</h2>

    


<div class="blog-post-date text-secondary">
    
        <time datetime="2017-09-21">Sep 21, 2017</time>
    
    
        by <span rel="author">Nitish Puri</span>
    
</div>

    
<div class="blog-post-tags text-secondary">
    <strong>Tags:</strong>
    
        <a class="btn btn-primary btn-small badge" href="../../../../tags/deep-learning">deep-learning</a>
    
        <a class="btn btn-primary btn-small badge" href="../../../../tags/notes">notes</a>
    
</div>


    

    <hr>
</header>
<article class="blog-post">

    <nav id="TableOfContents">
  <ul>
    <li><a href="#chapter-1-using-neural-nets-to-recognize-handwritten-digits">Chapter 1: Using neural nets to recognize handwritten digits</a>
      <ul>
        <li><a href="#perceptrons">Perceptrons</a></li>
        <li><a href="#sigmoid-neurons">Sigmoid Neurons</a></li>
        <li><a href="#the-architecture-of-neural-networks">The architecture of neural networks</a></li>
        <li><a href="#a-simple-network-to-classify-handwritten-digits">A simple network to classify handwritten digits</a></li>
        <li><a href="#learning-with-gradient-descent">Learning with gradient descent</a></li>
        <li><a href="#implementing-our-network-to-classify-digits">Implementing our network to classify digits</a></li>
        <li><a href="#towards-deep-learning">Towards Deep Learning</a></li>
      </ul>
    </li>
  </ul>
</nav>
    
    <p>Notes for the <a href="http://neuralnetworksanddeeplearning.com/index.html">book</a>.<br>
<a href="https://github.com/mnielsen/neural-networks-and-deep-learning">Source code</a> for the book.</p>
<h2 id="chapter-1-using-neural-nets-to-recognize-handwritten-digits">Chapter 1: Using neural nets to recognize handwritten digits</h2>
<h3 id="perceptrons">Perceptrons</h3>
<p><img src="../../../../images/nnfordl/perceptron1.png" alt="alt"><br>
$$\begin{eqnarray}
\mbox{output} &amp; = &amp; \left{ \begin{array}{ll}
0 &amp; \mbox{if } \sum_j w_j x_j \leq \mbox{ threshold} \<br>
1 &amp; \mbox{if } \sum_j w_j x_j &gt; \mbox{ threshold}
\end{array} \right.
\tag{1}\end{eqnarray}$$</p>
<h4 id="network-of-perceptrons">Network of perceptrons</h4>
<p><img src="../../../../images/nnfordl/perceptron2.png" alt="alt"></p>
<p>First, simplify notation, $w \cdot x \equiv \sum_j w_j x_j$<br>
Move, <em>threshold</em> into the network as <em>bias</em>, $b \equiv -\mbox{threshold}$<br>
$$\begin{eqnarray}
\mbox{output} = \left{ 
\begin{array}{ll} 
0 &amp; \mbox{if } w\cdot x + b \leq 0 \<br>
1 &amp; \mbox{if } w\cdot x + b &gt; 0
\end{array}
\right.
\tag{2}\end{eqnarray}$$</p>
<h4 id="neural-nets-as-logic-gates">Neural Nets as Logic Gates</h4>
<p>This network represent a <em>NAND</em> Gate<br>
<img src="../../../../images/nnfordl/perceptron3.png" alt="alt"></p>
<p><em>NAND</em> gates can do arbitrary computations,<br>
<img src="../../../../images/nnfordl/perceptron4.png" alt="alt"></p>
<p>And so can the perceptrons,<br>
<img src="../../../../images/nnfordl/perceptron5.png" alt="alt"></p>
<p>This is reassuring, as this shows that perceptron can be as powerful as any computing device. Bit is disappointing as it makes the perceptrons just another type of <em>NAND</em>.</p>
<p>However, these <em>perceptrons</em> can <em>learn</em>.</p>
<h3 id="sigmoid-neurons">Sigmoid Neurons</h3>
<p>We want our network to do this,<br>
<img src="../../../../images/nnfordl/perceptron6.png" alt="alt"></p>
<p>we would be able to <em>learn</em> if we repeat this process gradually <em>improving</em> our weights.</p>
<p>However, perceptrons don&rsquo;t behave that way, changing the weights or bias may only completely flip the output, say 0 to 1. This makes it difficult to do gradual improvements to our network.</p>
<p>Enter <em>sigmoid</em>,<br>
$$\begin{eqnarray} 
\sigma(z) \equiv \frac{1}{1+e^{-z}}.
\tag{3}\end{eqnarray}$$</p>
<p>and, $\sigma(z) \in [0,1]$</p>
<p>Output of sigmoid neuron $ = \sigma(w \cdot x+b)$</p>
<p><img src="../../../../images/nnfordl/perceptron7.png" alt="alt"></p>
<p>Which is a smoothed out version of the <em>step</em> function represented by <em>perceptrons</em>.</p>
<p>Using come calculus,<br>
$$\begin{eqnarray} 
\Delta \mbox{output} \approx \sum_j \frac{\partial , \mbox{output}}{\partial w_j}
\Delta w_j + \frac{\partial , \mbox{output}}{\partial b} \Delta b,
\tag{5}\end{eqnarray}$$</p>
<p>$\Delta \mbox{output}$ is a <em>linear</em> function with respect to $\Delta w_j$ and $\Delta b$.<br>
This makes it easier to figure out how to change the weights to achieve some output.</p>
<h3 id="the-architecture-of-neural-networks">The architecture of neural networks</h3>
<p><em>Feedforward</em> neural network<br>
<img src="../../../../images/nnfordl/perceptron7.png" alt="alt"></p>
<h3 id="a-simple-network-to-classify-handwritten-digits">A simple network to classify handwritten digits</h3>
<p><img src="../../../../images/nnfordl/mnist1.png" alt="alt"></p>
<h3 id="learning-with-gradient-descent">Learning with gradient descent</h3>
<p>The <a href="http://yann.lecun.com/exdb/mnist/">dataset</a>.</p>
<p>We use the following <em>quadratic cost function</em>, also called the <em>mean squared error</em> or just <em>MSE</em>,<br>
$$\begin{eqnarray}  C(w,b) \equiv
\frac{1}{2n} \sum_x | y(x) - a|^2.
\tag{6}\end{eqnarray}$$</p>
<p>To minimize this function, we use <em>gradient descent</em>,</p>
<p><em>Gradient descent with two variables,</em><br>
<img src="../../../../images/nnfordl/mnist2.png" alt="alt"></p>
<p>We could use calculus to find the minima analytically, but it would become a nightmare as soon as the number of variables go up.</p>
<p>So, let&rsquo;s start rolling a ball down the valley,..</p>
<p>$$\begin{eqnarray} 
\Delta C \approx \frac{\partial C}{\partial v_1} \Delta v_1 +
\frac{\partial C}{\partial v_2} \Delta v_2.
\tag{7}\end{eqnarray}$$</p>
<p>We also define a <em>gradient</em> vector $\nabla C$,<br>
$$\begin{eqnarray} 
\nabla C \equiv \left( \frac{\partial C}{\partial v_1}, 
\frac{\partial C}{\partial v_2} \right)^T.
\tag{8}\end{eqnarray}$$</p>
<p>With this defined, we can rewrite,<br>
$$\begin{eqnarray} 
\Delta C \approx \nabla C \cdot \Delta v.
\tag{9}\end{eqnarray}$$</p>
<p>This equation lets us choose $\Delta v$ so as to make $\Delta C$ negative.<br>
$$\begin{eqnarray} 
\Delta v = -\eta \nabla C,
\tag{10}\end{eqnarray}$$</p>
<p>where $\eta$ is a small, positive parameter (known as the <em>learning rate</em>).</p>
<p>Then, we can combine $(9)$ and $(10)$ to give, $\Delta C \approx -\eta \nabla C \cdot \nabla C = -\eta |\nabla C|^2$.</p>
<p>So, we will use $(10)$ to compute $\Delta v$,<br>
$$\begin{eqnarray}
v \rightarrow v' = v -\eta \nabla C.
\tag{11}\end{eqnarray}$$</p>
<p>doing this until - we hope - we reach a global minimum.</p>
<p>This can also be written as,<br>
$$\begin{eqnarray}
w_k &amp; \rightarrow &amp; w_k' = w_k-\eta \frac{\partial C}{\partial w_k} \tag{16}\<br>
b_l &amp; \rightarrow &amp; b_l' = b_l-\eta \frac{\partial C}{\partial b_l}.
\tag{17}\end{eqnarray}$$</p>
<p>Calculating all the gradients can become slow,</p>
<p>So we do that in batches, using <em>stochastic gradient descent</em>,<br>
We randomly choose a <em>mini-batch</em> of samples from the training inputs, $X_1, X_2, \ldots,X_m$<br>
$$\begin{eqnarray}
\frac{\sum_{j=1}^m \nabla C_{X_{j}}}{m} \approx \frac{\sum_x \nabla C_x}{n} = \nabla C,
\tag{18}\end{eqnarray}$$</p>
<p>So now our update rule becomes,<br>
$$\begin{eqnarray} 
w_k &amp; \rightarrow &amp; w_k' = w_k-\frac{\eta}{m}
\sum_j \frac{\partial C_{X_j}}{\partial w_k} \tag{20}\</p>
<p>b_l &amp; \rightarrow &amp; b_l' = b_l-\frac{\eta}{m}
\sum_j \frac{\partial C_{X_j}}{\partial b_l},
\tag{21}\end{eqnarray}$$</p>
<p>We do this update for all the training data, dividing it into batches. This completes a single <em>epoch</em>.</p>
<h3 id="implementing-our-network-to-classify-digits">Implementing our network to classify digits</h3>
<p>Clone the repo,</p>
<pre><code>git clone https://github.com/mnielsen/neural-networks-and-deep-learning.git
</code></pre><p>We will be using 10,000 images from the test set as our <em>validation set</em>.<br>
This will be used for <em>hyper-parameter tuning</em>.</p>
<h4 id="the-network-class">The <em>Network</em> class</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Network</span>(object):

    <span style="color:#66d9ef">def</span> __init__(self, sizes):
        self<span style="color:#f92672">.</span>num_layers <span style="color:#f92672">=</span> len(sizes)    <span style="color:#75715e"># Number of layers</span>
        self<span style="color:#f92672">.</span>sizes <span style="color:#f92672">=</span> sizes              <span style="color:#75715e"># Number of neurons in the respective layer</span>
        self<span style="color:#f92672">.</span>biases <span style="color:#f92672">=</span> [np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randn(y, <span style="color:#ae81ff">1</span>) <span style="color:#66d9ef">for</span> y <span style="color:#f92672">in</span> sizes[<span style="color:#ae81ff">1</span>:]]
        self<span style="color:#f92672">.</span>weights <span style="color:#f92672">=</span> [np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randn(y, x) 
                        <span style="color:#66d9ef">for</span> x, y <span style="color:#f92672">in</span> zip(sizes[:<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>], sizes[<span style="color:#ae81ff">1</span>:])]
</code></pre></div><p>To create a Nueral Net with 2 neurons in the first layer, 3 neurons in the second layer, and 1 neuron in the final layer,</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">net <span style="color:#f92672">=</span> Network([<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">1</span>])
</code></pre></div><p>It then generates random initial <em>bias</em> and <em>weights</em> for the layers.</p>
<p>We can calculate the activations of a given layer by,<br>
$$\begin{eqnarray} 
a' = \sigma(w a + b).
\tag{22}\end{eqnarray}$$</p>
<p>Here we are <em>vectorizing</em> out operations to denote(and compute) them compactly.
So, we can define the <em>sigmoid</em> activation in our code,</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">sigmoid</span>(z):
    <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">1.0</span><span style="color:#f92672">/</span>(<span style="color:#ae81ff">1.0</span><span style="color:#f92672">+</span>np<span style="color:#f92672">.</span>exp(<span style="color:#f92672">-</span>z))
</code></pre></div><p>and the <em>feedforward</em> function in the <code>Network</code> class,</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">feedforward</span>(self, a):
    <span style="color:#e6db74">&#34;&#34;&#34;Return the output of the network, if `a` is the input&#34;&#34;&#34;</span>
    <span style="color:#66d9ef">for</span> b, w <span style="color:#f92672">in</span> zip(self<span style="color:#f92672">.</span>biases, self<span style="color:#f92672">.</span>weights):
        a <span style="color:#f92672">=</span> sigmoid(np<span style="color:#f92672">.</span>dot(w, a) <span style="color:#f92672">+</span> b)
    <span style="color:#66d9ef">return</span> a
</code></pre></div><p>Now, the main thing that we want our network to do is to learn, so we&rsquo;ll define an <code>SGD</code> method</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">SGD</span>(self, trainig_data, epochs, mini_batch_size, eta, test_data<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>):
    <span style="color:#e6db74">&#34;&#34;&#34;Train the neural network using mini-batch stochastic gradient descent. 
</span><span style="color:#e6db74">    The &#34;training_data&#34; is a list of tuples &#34;(x,y)&#34; representing the training inputs
</span><span style="color:#e6db74">    and the desired outputs. If &#34;test_data&#34; is provided then the network will be evaluated 
</span><span style="color:#e6db74">    against then test data after each epoch. This is good for tracking the progress, 
</span><span style="color:#e6db74">    but slows things down&#34;&#34;&#34;</span>

    <span style="color:#66d9ef">if</span> test_data: n_test <span style="color:#f92672">=</span> len(test_data)
    n <span style="color:#f92672">=</span> len(trainig_data)
    <span style="color:#66d9ef">for</span> j <span style="color:#f92672">in</span> xrange(epochs):
        random<span style="color:#f92672">.</span>shuffle(trainig_data)
        mini_batches <span style="color:#f92672">=</span> [
            trainig_data[k:k<span style="color:#f92672">+</span>mini_batch_size]
            <span style="color:#66d9ef">for</span> k <span style="color:#f92672">in</span> xrange(<span style="color:#ae81ff">0</span>,n,mini_batch_size)]

        <span style="color:#66d9ef">for</span> mini_batch <span style="color:#f92672">in</span> mini_batches:
            self<span style="color:#f92672">.</span>update_mini_batch(mini_batch, eta)
        <span style="color:#66d9ef">if</span> test_data:
            print (<span style="color:#e6db74">&#34;Epoch </span><span style="color:#e6db74">{0}</span><span style="color:#e6db74">:</span><span style="color:#e6db74">{1}</span><span style="color:#e6db74">/</span><span style="color:#e6db74">{2}</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">.</span>format(j, self<span style="color:#f92672">.</span>evaluate(test_data), n_test))
        <span style="color:#66d9ef">else</span>:
            print(<span style="color:#e6db74">&#34;Epocj </span><span style="color:#e6db74">{0}</span><span style="color:#e6db74"> complete.&#34;</span><span style="color:#f92672">.</span>format(j))
</code></pre></div><p>In the above code the training data is randomly divided into mini batches and for each batch the gradient step is applied using <code>self.update_mini_batch(mini_batch, eta)</code>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">update_mini_batch</span>(self, mini_batch, eta):
    <span style="color:#e6db74">&#34;&#34;&#34;Update the network&#39;s weights and biases by applying
</span><span style="color:#e6db74">    gradient descent using backpropogation to a single  mini batch.
</span><span style="color:#e6db74">    The &#34;mini_batch&#34; is a list of tuples &#34;(x,y)&#34;, and &#34;eta&#34; is 
</span><span style="color:#e6db74">    the learning_rate.&#34;&#34;&#34;</span> 

    nabla_b <span style="color:#f92672">=</span> [np<span style="color:#f92672">.</span>zeros(b<span style="color:#f92672">.</span>shape) <span style="color:#66d9ef">for</span> b <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>biases]
    nabla_w <span style="color:#f92672">=</span> [np<span style="color:#f92672">.</span>zeros(w<span style="color:#f92672">.</span>shape) <span style="color:#66d9ef">for</span> w <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>weights]

    <span style="color:#66d9ef">for</span> x, y <span style="color:#f92672">in</span> mini_batch:
        delta_nabla_b, delta_nabla_w <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>backprop(x, y)
        nabla_b <span style="color:#f92672">=</span> [nb <span style="color:#f92672">+</span> dnb <span style="color:#66d9ef">for</span> nb, dnb <span style="color:#f92672">in</span> zip(nabla_b, delta_nabla_b)]
        nabla_w <span style="color:#f92672">=</span> [nw <span style="color:#f92672">+</span> dnw <span style="color:#66d9ef">for</span> nw, dnw <span style="color:#f92672">in</span> zip(nabla_w, delta_nabla_w)]

    self<span style="color:#f92672">.</span>weights <span style="color:#f92672">=</span> [w<span style="color:#f92672">-</span>(eta<span style="color:#f92672">/</span>len(mini_batch))<span style="color:#f92672">*</span>nw 
        <span style="color:#66d9ef">for</span> w, nw <span style="color:#f92672">in</span> zip(self<span style="color:#f92672">.</span>weights, nabla_w)]
    self<span style="color:#f92672">.</span>biases <span style="color:#f92672">=</span> [b<span style="color:#f92672">-</span>(eta<span style="color:#f92672">/</span>len(mini_batch))<span style="color:#f92672">*</span>nb 
        <span style="color:#66d9ef">for</span> b, nb <span style="color:#f92672">in</span> zip(self<span style="color:#f92672">.</span>biases, nabla_b)]
</code></pre></div><p>Here, most of the work is done by the line,(which is explained in the next article in the series)</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">       delta_nabla_b, delta_nabla_w <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>backprop(x, y)
</code></pre></div><p>We can now load some data using the helper scripts in the repo,</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">&gt;&gt;&gt;</span> <span style="color:#f92672">import</span> mnist_loader
<span style="color:#f92672">&gt;&gt;&gt;</span> training_data, validation_data, test_data <span style="color:#f92672">=</span> \
<span style="color:#f92672">...</span> mnist_loader<span style="color:#f92672">.</span>load_data_wrapper()
</code></pre></div><p>And then create a network,</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">&gt;&gt;&gt;</span> <span style="color:#f92672">import</span> network
<span style="color:#f92672">&gt;&gt;&gt;</span> net <span style="color:#f92672">=</span> network<span style="color:#f92672">.</span>Network([<span style="color:#ae81ff">784</span>, <span style="color:#ae81ff">30</span>, <span style="color:#ae81ff">10</span>])
</code></pre></div><p>Finally, we can use <code>SGD</code> to learn from the MNIST data,</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">&gt;&gt;&gt;</span> net<span style="color:#f92672">.</span>SGD(trainig_data, epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">30</span>, mini_batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>, eta<span style="color:#f92672">=</span><span style="color:#ae81ff">3.0</span>, test_data<span style="color:#f92672">=</span>validation_data)
</code></pre></div><p>Here is the output you should expect,</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">Epoch <span style="color:#ae81ff">0</span>: <span style="color:#ae81ff">9129</span> <span style="color:#f92672">/</span> <span style="color:#ae81ff">10000</span>
Epoch <span style="color:#ae81ff">1</span>: <span style="color:#ae81ff">9295</span> <span style="color:#f92672">/</span> <span style="color:#ae81ff">10000</span>
Epoch <span style="color:#ae81ff">2</span>: <span style="color:#ae81ff">9348</span> <span style="color:#f92672">/</span> <span style="color:#ae81ff">10000</span>
<span style="color:#f92672">...</span>
Epoch <span style="color:#ae81ff">27</span>: <span style="color:#ae81ff">9528</span> <span style="color:#f92672">/</span> <span style="color:#ae81ff">10000</span>
Epoch <span style="color:#ae81ff">28</span>: <span style="color:#ae81ff">9542</span> <span style="color:#f92672">/</span> <span style="color:#ae81ff">10000</span>
Epoch <span style="color:#ae81ff">29</span>: <span style="color:#ae81ff">9534</span> <span style="color:#f92672">/</span> <span style="color:#ae81ff">10000</span>
</code></pre></div><p>That is <em>95.42</em> percent accuracy in <em>28 epochs</em>.</p>
<p><em>Tuning the hyperparameters</em></p>
<p>This can be challenging. The art of debugging is required here.</p>
<p>The current state-of-the-art for this dataset is <em>99.79</em> percent.</p>
<h3 id="towards-deep-learning">Towards Deep Learning</h3>
<p>How does the network does what it does?</p>
<p>Possibly by, <em>breaking down the problem into subproblems and finding those answers</em>.</p>
<p>But, this <em>breaking down</em> is done by the network <em>automatically</em> while learning, 
and we don&rsquo;t really have a say in it.</p>


    

    <footer>


    <h4>See also</h4>
    <ul>
        
            <li><a href="../../../../books/programming/neuralnets/neural-networks-and-deep-learning-4/">A visual proof that neural networks can compute any function, Micheal Nelson</a></li>
        
            <li><a href="../../../../books/programming/neuralnets/neural-networks-and-deep-learning-6/">Deep Learning, Micheal Nelson</a></li>
        
            <li><a href="../../../../books/programming/neuralnets/neural-networks-and-deep-learning-2/">How the backpropogation algorithm works, Micheal Nelson</a></li>
        
            <li><a href="../../../../books/programming/neuralnets/neural-networks-and-deep-learning-3/">Improving the way neural networks learn, Micheal Nelson</a></li>
        
            <li><a href="../../../../research/machine-intelligence/nn-for-cv-1/">Neural Networks for Computer Vision</a></li>
        
    </ul>

</footer>

</article>



        </div>

        <aside class="col-12 col-lg-3 ml-auto blog-sidebar">
    
        


<section>
    <h4>Recent Posts</h4>
    <ol class="list-unstyled">
        
        <li>
            <a href="../../../../books/programming/the-little-schemer/">The Little Schemer</a>
        </li>
        
        <li>
            <a href="../../../../books/programming/game-engine-architecture/">Game Engine Architecture</a>
        </li>
        
        <li>
            <a href="../../../../books/programming/neuralnets/neural-networks-and-deep-learning-7/">Is there a simple algorithm for intelligence?, Micheal Nelson</a>
        </li>
        
        <li>
            <a href="../../../../books/programming/neuralnets/neural-networks-and-deep-learning-6/">Deep Learning, Micheal Nelson</a>
        </li>
        
        <li>
            <a href="../../../../books/programming/neuralnets/neural-networks-and-deep-learning-5/">Why are deep neural networks hard to train?, Micheal Nelson</a>
        </li>
        
    </ol>
</section>


    
    
        <section>
    
        
    
        
        <h4>Categories</h4>
        <p>
            
            <a class="btn btn-primary btn-small badge" href="../../../../categories/books">books</a>
            
            <a class="btn btn-primary btn-small badge" href="../../../../categories/courses">courses</a>
            
            <a class="btn btn-primary btn-small badge" href="../../../../categories/machine-intelligence">machine-intelligence</a>
            
            <a class="btn btn-primary btn-small badge" href="../../../../categories/notes">notes</a>
            
        </p>
        
    
        
        <h4>Tags</h4>
        <p>
            
            <a class="btn btn-primary btn-small badge" href="../../../../tags/algorithms">algorithms</a>
            
            <a class="btn btn-primary btn-small badge" href="../../../../tags/architecture">architecture</a>
            
            <a class="btn btn-primary btn-small badge" href="../../../../tags/biorobots">biorobots</a>
            
            <a class="btn btn-primary btn-small badge" href="../../../../tags/book">book</a>
            
            <a class="btn btn-primary btn-small badge" href="../../../../tags/data-science">data-science</a>
            
            <a class="btn btn-primary btn-small badge" href="../../../../tags/deep-learning">deep-learning</a>
            
            <a class="btn btn-primary btn-small badge" href="../../../../tags/design">design</a>
            
            <a class="btn btn-primary btn-small badge" href="../../../../tags/game-engine">game-engine</a>
            
            <a class="btn btn-primary btn-small badge" href="../../../../tags/graphics">graphics</a>
            
            <a class="btn btn-primary btn-small badge" href="../../../../tags/image-segmentation">image-segmentation</a>
            
            <a class="btn btn-primary btn-small badge" href="../../../../tags/kuka">kuka</a>
            
            <a class="btn btn-primary btn-small badge" href="../../../../tags/lisp">lisp</a>
            
            <a class="btn btn-primary btn-small badge" href="../../../../tags/machine-intelligence">machine-intelligence</a>
            
            <a class="btn btn-primary btn-small badge" href="../../../../tags/notes">notes</a>
            
            <a class="btn btn-primary btn-small badge" href="../../../../tags/opengl">opengl</a>
            
            <a class="btn btn-primary btn-small badge" href="../../../../tags/philosophy">philosophy</a>
            
            <a class="btn btn-primary btn-small badge" href="../../../../tags/programming">programming</a>
            
            <a class="btn btn-primary btn-small badge" href="../../../../tags/projects">projects</a>
            
            <a class="btn btn-primary btn-small badge" href="../../../../tags/robotics">robotics</a>
            
            <a class="btn btn-primary btn-small badge" href="../../../../tags/scheme">scheme</a>
            
            <a class="btn btn-primary btn-small badge" href="../../../../tags/style-transfer">style-transfer</a>
            
            <a class="btn btn-primary btn-small badge" href="../../../../tags/udacity">udacity</a>
            
        </p>
        
    
        
    
</section>

    

    

</aside>


      </div>
    </div>
    

    
      
    

    
    
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta1/dist/js/bootstrap.bundle.min.js" integrity="sha384-ygbV9kiqUc6oa4msXn9868pTtWMgiQaeYH7/t7LECLbyPA2x65Kgf80OJFdroafW" crossorigin="anonymous"></script>
  </body>
</html>
