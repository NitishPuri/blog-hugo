<!doctype html><html lang=en>
<head>
<meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="ie=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta property="og:title" content="Elements Of Statistical Learning, Part 1">
<meta property="og:description" content="Chapter 1: Introduction  Motivation towards statistical learning and belief in data. What&rsquo;s next.  Chapter 2: Overview of Supervised Learning  Variable types and terminology  Quantitative vs Qualitative output. Regression and Classification   Simple approaches : Least Squares and Nearest Neighbors  Linear Models and Least Squares
$\hat Y = \hat \beta_0 + \sum_{j=1}^pX_j\hat\beta_j$  Least squares by solving normal equations.   Nearest Neighbor Methods  Voronoi tessellation   From Least Squares to Nearest Neighbors   Statistical Decision Theory Local Methods in High Dimensions  **The curse of Dimensionality,Bellman **     Statistical Models, Supervised Learning and Function Approximation  A Statistical Model for the Joint Distribution Pr(X, Y ) Supervised Learning Function Approximation   Structured Regression Models  Difficulty of the Problem   Classes of Restricted Estimators  Roughness Penalty and Bayesian Methods  regularization   Kernel Methods and Local Regression Basis Functions and Dictionary Methods   Model Selection and the Bias–Variance Tradeoff">
<meta property="og:type" content="article">
<meta property="og:url" content="https://nitishpuri.github.io/blog-hugo/books/programming/ele/elements-of-statistical-learning-part-1/"><meta property="og:image" content="https://nitishpuri.github.io/images/site-feature-image.jpg"><meta property="article:section" content="books">
<meta property="article:modified_time" content="2017-08-09T00:00:00+00:00">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://nitishpuri.github.io/images/site-feature-image.jpg">
<meta name=twitter:title content="Elements Of Statistical Learning, Part 1">
<meta name=twitter:description content="Chapter 1: Introduction  Motivation towards statistical learning and belief in data. What&rsquo;s next.  Chapter 2: Overview of Supervised Learning  Variable types and terminology  Quantitative vs Qualitative output. Regression and Classification   Simple approaches : Least Squares and Nearest Neighbors  Linear Models and Least Squares
$\hat Y = \hat \beta_0 + \sum_{j=1}^pX_j\hat\beta_j$  Least squares by solving normal equations.   Nearest Neighbor Methods  Voronoi tessellation   From Least Squares to Nearest Neighbors   Statistical Decision Theory Local Methods in High Dimensions  **The curse of Dimensionality,Bellman **     Statistical Models, Supervised Learning and Function Approximation  A Statistical Model for the Joint Distribution Pr(X, Y ) Supervised Learning Function Approximation   Structured Regression Models  Difficulty of the Problem   Classes of Restricted Estimators  Roughness Penalty and Bayesian Methods  regularization   Kernel Methods and Local Regression Basis Functions and Dictionary Methods   Model Selection and the Bias–Variance Tradeoff">
<link rel=canonical href=https://nitishpuri.github.io/blog-hugo/books/programming/ele/elements-of-statistical-learning-part-1/>
<title>
Elements Of Statistical Learning, Part 1 | Blog - Nitish Puri
</title>
<link href=https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta1/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-giJF6kkoqNQ00vy+HMDP7azOuL0xtbfIcaT9wjKHr8RbDVddVHyTfAAsrekwKmP1 crossorigin=anonymous>
<link href=../../../../blog-hugo/css/style.css rel=stylesheet>
<script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:['script','noscript','style','textarea','pre']}},window.addEventListener('load',a=>{document.querySelectorAll("mjx-container").forEach(function(a){a.parentElement.classList+='has-jax'})})</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
</head>
<body>
<header class=blog-header>
<nav class="navbar navbar-expand-md navbar-light bg-light">
<div class=container-fluid>
<a class=navbar-brand href=../../../../blog-hugo>
<img src=https://getbootstrap.com/docs/4.1/assets/brand/bootstrap-solid.svg width=30 height=30 class="d-inline-block align-top" alt>
Blog - Nitish Puri
</a>
<button class=navbar-toggler type=button data-bs-toggle=collapse data-bs-target=#navbarNav aria-controls=navbarNav aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span>
</button>
<div class="collapse navbar-collapse justify-content-between" id=navbarNav>
<ul class=navbar-nav>
<li class=nav-item>
<a class=nav-link href=../../../../blog-hugo/blog-hugo/books/></a>
</li>
<li class=nav-item>
<a class=nav-link href=../../../../blog-hugo/blog-hugo/research/></a>
</li>
<li class=nav-item>
<a class=nav-link href=../../../../blog-hugo/blog-hugo/about/>About</a>
</li>
</ul>
</div>
</div>
</nav>
</header>
<div class=container>
<div class=row>
<div class="col-12 col-lg-8 blog-main"><nav class="breadcrumb container" aria-label=breadcrumb>
<ol class=breadcrumb>
<li class=breadcrumb-item> <a href=https://nitishpuri.github.io/blog-hugo/>Home</a> </li>
<li class=breadcrumb-item> <a href=https://nitishpuri.github.io/blog-hugo/books/>Books</a> </li>
<li class=breadcrumb-item> <a href=https://nitishpuri.github.io/blog-hugo/books/programming/>Programming</a> </li>
<li class=breadcrumb-item> <a href=https://nitishpuri.github.io/blog-hugo/books/programming/ele/>Elements Of Statistical Learning by Trevor Hastie, Robert Tibshirani, Jerome Friedman</a> </li>
<li class="breadcrumb-item active" aria-current=page> Elements Of Statistical Learning, Part 1</li>
</ol>
</nav>
<header>
<h2 class=blog-post-title>
<a class="text-dark text-decoration-none" href=../../../../blog-hugo/books/programming/ele/elements-of-statistical-learning-part-1/>Elements Of Statistical Learning, Part 1</a>
</h2>
<div class="blog-post-date text-secondary">
<time datetime=2017-08-09>Aug 9, 2017</time>
by <span rel=author>Nitish Puri</span>
</div>
<div class="blog-post-tags text-secondary">
<strong>Tags:</strong>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/tags/data-science>data-science</a>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/tags/notes>notes</a>
</div>
<hr>
</header>
<article class=blog-post>
<nav id=TableOfContents>
<ul>
<li><a href=#chapter-1-introduction>Chapter 1: Introduction</a></li>
<li><a href=#chapter-2-overview-of-supervised-learning>Chapter 2: Overview of Supervised Learning</a></li>
<li><a href=#chapter-3-linear-methods-of-regression>Chapter 3: Linear Methods Of Regression</a></li>
<li><a href=#chapter-4-linear-methods-of-classification>Chapter 4: Linear Methods of Classification</a></li>
<li><a href=#chapter-5-basis-expansions-and-regularization>Chapter 5: Basis Expansions and Regularization</a></li>
<li><a href=#chapter-6-kernel-smoothing-methods>Chapter 6: Kernel Smoothing Methods</a></li>
</ul>
</nav>
<h2 id=chapter-1-introduction>Chapter 1: Introduction</h2>
<ul>
<li>Motivation towards statistical learning and belief in data.</li>
<li>What&rsquo;s next.</li>
</ul>
<h2 id=chapter-2-overview-of-supervised-learning>Chapter 2: Overview of Supervised Learning</h2>
<ul>
<li>Variable types and terminology
<ul>
<li>Quantitative vs Qualitative output.</li>
<li>Regression and Classification</li>
</ul>
</li>
<li>Simple approaches : Least Squares and Nearest Neighbors
<ul>
<li>Linear Models and Least Squares<br>
$\hat Y = \hat \beta_0 + \sum_{j=1}^pX_j\hat\beta_j$
<ul>
<li>Least squares by solving <em>normal</em> equations.</li>
</ul>
</li>
<li>Nearest Neighbor Methods
<ul>
<li><em>Voronoi tessellation</em></li>
</ul>
</li>
<li>From Least Squares to Nearest Neighbors</li>
</ul>
</li>
<li>Statistical Decision Theory</li>
<li>Local Methods in High Dimensions
<ul>
<li>**The curse of Dimensionality,<em>Bellman</em> **</li>
</ul>
</li>
</ul>
<ul>
<li>Statistical Models, Supervised Learning and Function Approximation
<ul>
<li>A Statistical Model for the Joint Distribution Pr(X, Y )</li>
<li>Supervised Learning</li>
<li>Function Approximation</li>
</ul>
</li>
<li>Structured Regression Models
<ul>
<li>Difficulty of the Problem</li>
</ul>
</li>
<li>Classes of Restricted Estimators
<ul>
<li>Roughness Penalty and Bayesian Methods
<ul>
<li><em>regularization</em></li>
</ul>
</li>
<li>Kernel Methods and Local Regression</li>
<li>Basis Functions and Dictionary Methods</li>
</ul>
</li>
<li>Model Selection and the Bias–Variance Tradeoff<br>
<img src=../../../../images/esl/bias-var.png alt=Bias-Var></li>
</ul>
<h2 id=chapter-3-linear-methods-of-regression>Chapter 3: Linear Methods Of Regression</h2>
<ul>
<li>Introduction</li>
<li>Linear Regression Models and Least Squares
<ul>
<li>Solution from <em>normal</em> form</li>
<li>F statistic</li>
<li>Example : prostrate cancer</li>
<li>The Gauss-Markov Theorem
<ul>
<li>Proof that the Least Squares estimate for the parameters, $\beta$ has the least variance.</li>
</ul>
</li>
<li>Multiple Regression from Simple Univariate Regression
<img src=../../../../images/esl/alg3_1.png alt="Alg 3.1"></li>
<li>Multiple Outputs</li>
</ul>
</li>
<li>Subset Selection
<ul>
<li>Best-Subset Selection</li>
<li>Forward and Backward-Stepwise Selection</li>
<li>Forward-Stagewise Selection
<img src=../../../../images/esl/subset-sel.png alt=alt></li>
<li>Example : Prostrate Cancer (Continued)</li>
</ul>
</li>
<li>Shrinkage Methods
<ul>
<li>Ridge Regression : L2 regularization</li>
<li>The Lasso : L1 regularization</li>
<li>Discussion : Subset Selection, Ridge Regression and the Lasso</li>
<li>Least Angle Regression</li>
</ul>
</li>
<li>Methods Using Derived Input Directions
<ul>
<li>Principal Components Regression</li>
<li>Partial Least Squares</li>
</ul>
</li>
<li>Discussion : A Comparison of Selection and Shrinkage Methods</li>
<li>Multiple Outcomes Shrinkage and Selection ☠</li>
<li>More on Lasso and Related Path Algorithms ☠
<ul>
<li>Incremental Forward Stagewise Regression</li>
<li>Piecewise-Linear Path Algorithms</li>
<li>The Dantzig selector</li>
<li>The Grouped Lasso</li>
<li>Further Properties of Lasso</li>
<li>Pathwise Coordinate Optimization</li>
</ul>
</li>
<li>Computational Considerations
<ul>
<li>Fitting is usually done using <em>Cholesky decomposition</em> of matrix $X^TX$.</li>
</ul>
</li>
</ul>
<h2 id=chapter-4-linear-methods-of-classification>Chapter 4: Linear Methods of Classification</h2>
<ul>
<li>Introduction</li>
<li>Linear Regression of an Indicator Matrix</li>
<li>Linear Discriminant Analysis
<ul>
<li>Regularized Discriminant Analysis</li>
<li>Computations for LDA</li>
<li>Reduced-Rank Linear Discriminant Analysis</li>
</ul>
</li>
<li>Logistic Regression
<ul>
<li>Fitting Logistic Regression Models</li>
<li>Example : South African Heart Disease</li>
<li>Quadratic Approximations and Inference</li>
<li>$L_1$ Regularized Logistic Regression</li>
<li>Logistic Regression or LDA ?</li>
</ul>
</li>
<li>Separating Hyperplanes
<ul>
<li>Rosenblatt’s Perceptron Learning Algorithm</li>
<li>Optimal Separating Hyperplanes ☠</li>
</ul>
</li>
</ul>
<h2 id=chapter-5-basis-expansions-and-regularization>Chapter 5: Basis Expansions and Regularization</h2>
<ul>
<li>Introduction</li>
<li>Piecewise Polynomials and Splines
<img src=../../../../images/esl/5_piecewise_1.png alt=alt>
<ul>
<li>Natural Cubic Splines</li>
<li>Example: South African Heart Disease (Continued)</li>
<li>Example: Phoneme Recognition</li>
</ul>
</li>
<li>Filtering and Feature Extraction</li>
<li>Smoothing Splines
<ul>
<li>Degrees of Freedom and Smoother Matrices</li>
</ul>
</li>
<li>Automatic Selection of the Smoothing Parameters
<ul>
<li>Fixing the Degrees of Freedom</li>
<li>The Bias–Variance Tradeoff</li>
</ul>
</li>
<li>Nonparametric Logistic Regression</li>
<li>Multidimensional Splines</li>
<li>Regularization and Reproducing Kernel Hilbert Spaces ☠
<ul>
<li>Spaces of Functions Generated by Kernels</li>
<li>Examples of RKHS</li>
<li>Penalized Polynomial Regression
<ul>
<li>Gaussian Radial Basis Functions</li>
<li>Support Vector Classifiers</li>
</ul>
</li>
</ul>
</li>
<li>Wavelet Smoothing ☠
<ul>
<li>Wavelet Smoothing and the Wavelet Transform</li>
<li>Adaptive Wavelet Filtering</li>
</ul>
</li>
</ul>
<h2 id=chapter-6-kernel-smoothing-methods>Chapter 6: Kernel Smoothing Methods</h2>
<ul>
<li>One-Dimensional Kernel Smoothers
<ul>
<li>Local Linear Regression</li>
<li>Local Polynomial Regression</li>
</ul>
</li>
<li>Selecting the Width of the Kernel</li>
<li>Local Regression in ${\mathbb R}^p$</li>
<li>Structured Local Regression Models in ${\mathbb R}^p$
<ul>
<li>Structured Kernels</li>
<li>Structured Regression Functions</li>
</ul>
</li>
<li>Kernel Density Estimation and Classification
<ul>
<li>Kernel Density Estimation</li>
<li>Kernel Density Classification</li>
<li>The Naive Bayes Classifier</li>
</ul>
</li>
<li>Radial Basis Functions and Kernels</li>
<li>Mixture Models for Density Estimation and Classification</li>
<li>Computational Considerations</li>
</ul>
<footer>
<h4>See also</h4>
<ul>
<li><a href=../../../../blog-hugo/books/programming/ele/elements-of-statistical-learning-part-2/>Elements Of Statistical Learning, Part 2</a></li>
<li><a href=../../../../blog-hugo/books/programming/ele/elements-of-statistical-learning-part-3/>Elements Of Statistical Learning, Part 3</a></li>
<li><a href=../../../../blog-hugo/books/programming/algorithms-design-manual/algorithms-design-manual8/>The Algorithms Design Manual, Set and String Problems</a></li>
<li><a href=../../../../blog-hugo/books/programming/algorithms-design-manual/algorithms-design-manual7/>The Algorithms Design Manual, Computational Geometry</a></li>
<li><a href=../../../../blog-hugo/books/programming/algorithms-design-manual/algorithms-design-manual6/>The Algorithms Design Manual, Graph Problems(Hard Problems)</a></li>
</ul>
</footer>
</article>
</div>
<aside class="col-12 col-lg-3 ml-auto blog-sidebar">
<section>
<h4>Recent Posts</h4>
<ol class=list-unstyled>
<li>
<a href=../../../../blog-hugo/books/programming/the-little-schemer/>The Little Schemer</a>
</li>
<li>
<a href=../../../../blog-hugo/books/programming/game-engine-architecture/>Game Engine Architecture</a>
</li>
<li>
<a href=../../../../blog-hugo/books/programming/neuralnets/neural-networks-and-deep-learning-7/>Is there a simple algorithm for intelligence?, Micheal Nelson</a>
</li>
<li>
<a href=../../../../blog-hugo/books/programming/neuralnets/neural-networks-and-deep-learning-6/>Deep Learning, Micheal Nelson</a>
</li>
<li>
<a href=../../../../blog-hugo/books/programming/neuralnets/neural-networks-and-deep-learning-5/>Why are deep neural networks hard to train?, Micheal Nelson</a>
</li>
</ol>
</section>
<section>
<h4>Categories</h4>
<p>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/categories/books>books</a>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/categories/courses>courses</a>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/categories/machine-intelligence>machine-intelligence</a>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/categories/notes>notes</a>
</p>
<h4>Tags</h4>
<p>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/tags/algorithms>algorithms</a>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/tags/architecture>architecture</a>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/tags/biorobots>biorobots</a>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/tags/book>book</a>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/tags/data-science>data-science</a>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/tags/deep-learning>deep-learning</a>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/tags/design>design</a>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/tags/game-engine>game-engine</a>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/tags/graphics>graphics</a>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/tags/image-segmentation>image-segmentation</a>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/tags/kuka>kuka</a>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/tags/lisp>lisp</a>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/tags/machine-intelligence>machine-intelligence</a>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/tags/notes>notes</a>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/tags/opengl>opengl</a>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/tags/philosophy>philosophy</a>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/tags/programming>programming</a>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/tags/projects>projects</a>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/tags/robotics>robotics</a>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/tags/scheme>scheme</a>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/tags/style-transfer>style-transfer</a>
<a class="btn btn-primary btn-small badge" href=../../../../blog-hugo/tags/udacity>udacity</a>
</p>
</section>
</aside>
</div>
</div>
<script src=https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta1/dist/js/bootstrap.bundle.min.js integrity=sha384-ygbV9kiqUc6oa4msXn9868pTtWMgiQaeYH7/t7LECLbyPA2x65Kgf80OJFdroafW crossorigin=anonymous></script>
</body>
</html>