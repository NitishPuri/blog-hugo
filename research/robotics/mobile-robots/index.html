<!doctype html><html lang=en>
<head>
<meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="ie=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta property="og:title" content="Mobile Robots">
<meta property="og:description" content="Very minimal notes on some papers or articles that I recently read. Mainly for logging purposes.
Kinematics tf: The Transform Library Tully Foote
Open Source Robotics Foundation
 The need for tf : to provide a standard way to keep track of coordinate frames and transform data within the entire system to a component user without requiring knowledge of all the coordinate frames. Broadcaster and Listner modules. Closely related to scene graphs.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://nitishpuri.github.io/blog-hugo/research/robotics/mobile-robots/"><meta property="og:image" content="https://nitishpuri.github.io/images/site-feature-image.jpg"><meta property="article:section" content="research">
<meta property="article:modified_time" content="2017-11-17T00:00:00+00:00">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://nitishpuri.github.io/images/site-feature-image.jpg">
<meta name=twitter:title content="Mobile Robots">
<meta name=twitter:description content="Very minimal notes on some papers or articles that I recently read. Mainly for logging purposes.
Kinematics tf: The Transform Library Tully Foote
Open Source Robotics Foundation
 The need for tf : to provide a standard way to keep track of coordinate frames and transform data within the entire system to a component user without requiring knowledge of all the coordinate frames. Broadcaster and Listner modules. Closely related to scene graphs.">
<link rel=canonical href=https://nitishpuri.github.io/blog-hugo/research/robotics/mobile-robots/>
<title>
Mobile Robots | Blog - Nitish Puri
</title>
<link href=https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta1/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-giJF6kkoqNQ00vy+HMDP7azOuL0xtbfIcaT9wjKHr8RbDVddVHyTfAAsrekwKmP1 crossorigin=anonymous>
<link href=../../../blog-hugo/css/style.css rel=stylesheet>
<script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:['script','noscript','style','textarea','pre']}},window.addEventListener('load',a=>{document.querySelectorAll("mjx-container").forEach(function(a){a.parentElement.classList+='has-jax'})})</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
</head>
<body>
<header class=blog-header>
<nav class="navbar navbar-expand-md navbar-light bg-light">
<div class=container-fluid>
<a class=navbar-brand href=../../../blog-hugo>
<img src=https://getbootstrap.com/docs/4.1/assets/brand/bootstrap-solid.svg width=30 height=30 class="d-inline-block align-top" alt>
Blog - Nitish Puri
</a>
<button class=navbar-toggler type=button data-bs-toggle=collapse data-bs-target=#navbarNav aria-controls=navbarNav aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span>
</button>
<div class="collapse navbar-collapse justify-content-between" id=navbarNav>
<ul class=navbar-nav>
<li class=nav-item>
<a class=nav-link href=../../../blog-hugo/blog-hugo/books/></a>
</li>
<li class=nav-item>
<a class=nav-link href=../../../blog-hugo/blog-hugo/research/></a>
</li>
<li class=nav-item>
<a class=nav-link href=../../../blog-hugo/blog-hugo/about/>About</a>
</li>
</ul>
</div>
</div>
</nav>
</header>
<div class=container>
<div class=row>
<div class="col-12 col-lg-8 blog-main"><nav class="breadcrumb container" aria-label=breadcrumb>
<ol class=breadcrumb>
<li class=breadcrumb-item> <a href=https://nitishpuri.github.io/blog-hugo/>Home</a> </li>
<li class=breadcrumb-item> <a href=https://nitishpuri.github.io/blog-hugo/research/>Researches</a> </li>
<li class=breadcrumb-item> <a href=https://nitishpuri.github.io/blog-hugo/research/robotics/>Robotics</a> </li>
<li class="breadcrumb-item active" aria-current=page> Mobile Robots</li>
</ol>
</nav>
<header>
<h2 class=blog-post-title>
<a class="text-dark text-decoration-none" href=../../../blog-hugo/research/robotics/mobile-robots/>Mobile Robots</a>
</h2>
<div class="blog-post-date text-secondary">
<time datetime=2017-11-17>Nov 17, 2017</time>
by <span rel=author>Nitish Puri</span>
</div>
<div class="blog-post-tags text-secondary">
<strong>Tags:</strong>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/tags/robotics>robotics</a>
</div>
<div class="blog-post-categories text-secondary">
<strong>Categories:</strong>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/categories/notes>notes</a>
</div>
<hr>
</header>
<article class=blog-post>
<nav id=TableOfContents>
<ul>
<li><a href=#kinematics>Kinematics</a>
<ul>
<li><a href=#tf><em>tf</em>: The Transform Library</a></li>
<li><a href=#6dof>Solving Kinematics Problems of a 6-DOF Robot Manipulator</a></li>
</ul>
</li>
<li><a href=#robot-grasping>Robot Grasping</a>
<ul>
<li><a href=#grasping1>Robotic Grasping and Contact: A Review</a></li>
<li><a href=#grasping2>Universal robotic gripper based on the jamming of granular material</a></li>
<li><a href=#grasping3>Dex-Net 2.0: Deep Learning to Plan Robust Grasps with Synthetic Point Clouds and Analytic Grasp Metrics</a></li>
<li><a href=#grasping4>Learning Hand-eye Coordination for Robotic Grasping with Deep Learning and Large-scale Data Collection</a></li>
</ul>
</li>
<li><a href=#mobile-robots>Mobile Robots</a>
<ul>
<li><a href=#mobile1>Modular and Reconfigurable Mobile Robots</a></li>
</ul>
</li>
<li><a href=#swarm-intelligence>Swarm Intelligence</a>
<ul>
<li><a href=#swarm1>Swarm Intelligence and Its Applications in Swarm Robotics</a></li>
</ul>
</li>
</ul>
</nav>
<p>Very minimal notes on some papers or articles that I recently read. Mainly for logging purposes.</p>
<h2 id=kinematics>Kinematics</h2>
<h3 id=tf><em>tf</em>: The Transform Library</h3>
<p><em>Tully Foote</em><br>
<em>Open Source Robotics Foundation</em></p>
<ul>
<li>The need for <em>tf</em> : to provide a standard way to keep track of coordinate frames and transform data within the entire system to a component user without requiring knowledge of all the coordinate frames.</li>
<li><em>Broadcaster</em> and <em>Listner</em> modules.</li>
<li>Closely related to <em>scene graphs</em>.<br>
<img src=../../../images/papers/tf1.jpg alt=alt></li>
<li>There are several different sources of information regarding various coordinate frames in a system, coming from sensors connected to hardware. This data can come at different frequencies.</li>
<li><em>tf</em> must accept asynchronous inputs and be robust to delayed or lost information.</li>
<li>Must be robust to a distributed computing environment with unreliable networking and non negligible latency.</li>
<li>Ability to dynamically change the relationship between frames to account for dynamic/varying structure.</li>
<li><strong>Design</strong></li>
<li>Transforms and frames are represented as a graph with transforms as edges and frames as nodes.</li>
<li>The graphs can be disconnected, and must be directed ,acyclic and quickly searchable.</li>
<li>Limiting the graphs to trees enables this.</li>
<li>Difference from scene graphs: they are made to be iterated across periodically, while <em>tf</em> is designed to be queried for values asynchronously.</li>
<li>History is also required.</li>
<li>This data collectively is called a <em>Stamp</em>.</li>
<li><em>Broadcaster</em> broadcasts messages every time an update is heard about a specific transform.</li>
<li><em>Listner</em> collects the values and interpolates using SLERP, without assuming the presence of a future frame.</li>
<li>The interpolation is a critical ability, as it allows the system to be asynchronous and robust to lost packets.</li>
<li><em>Transform Computation</em> using chaining. $T_c^a=T_a^b \times T_b^c$.</li>
<li><strong>Strengths :</strong> <em>Efficiency, Flexibility</em></li>
<li><strong>Extensions :</strong></li>
<li>Support for velocity.</li>
<li>Transforming data in time.</li>
</ul>
<h3 id=6dof>Solving Kinematics Problems of a 6-DOF Robot Manipulator</h3>
<p><em>Computer Science Department, The University of Georgia : 2015</em><br>
<a href="https://www.google.com/url?q=https%3A%2F%2Fwww.researchgate.net%2Ffile.PostFileLoader.html%3Fid%3D57cd4b20615e274c742de265%26assetKey%3DAS%253A402906919522304%25401473071903479&sa=D&sntz=1&usg=AFQjCNF8-U-44kQLISfar99yrIFTYElU9w"><em>Source</em></a></p>
<ul>
<li>An analytical approach for solving forward kinematics problem of a serial robot manipulator with six degrees of freedom and a specific combination of joints and links to formulate the position of gripper by a given set of joint angles.<br>
<img src=../../../images/papers/kuka_1.jpg alt=alt></li>
<li>The functional state of each joint related to its successive joint in the design of this robot is as follows:<br>
$$ R_1 \bot R_2 \parallel R_3 \bot R_4 \bot R_5 \bot R_6 $$<br>
in which $R$ indicates a revolute joint and the indices describe the position of the joint relative to the base of the robot.</li>
<li>Uses D-H parameter convention for assigning coordinate frames.</li>
<li>D-H parameter analysis for Kuka KR60 can be found <a href=%7Bfilename%7Dkuka-kinematics.md>here</a></li>
</ul>
<h2 id=robot-grasping>Robot Grasping</h2>
<h3 id=grasping1>Robotic Grasping and Contact: A Review</h3>
<p><a href=https://pdfs.semanticscholar.org/54f8/8557554d9a4e517f301cb170bd50cbe4cfc9.pdf><em>Source</em></a></p>
<ul>
<li>Survey of work done in last two decades.</li>
<li>Functions of Human hand
<ul>
<li>Explore : <em>haptics</em></li>
<li>Restrain : <em>fixturing</em></li>
<li>Manipulation : <em>dexterous manipulation</em></li>
</ul>
</li>
<li>Closure properties of grasps
<ul>
<li>Contact modelling the grap</li>
</ul>
</li>
<li>Force Analysis</li>
<li>Contact model
<ul>
<li>Kinematics of contact</li>
<li>Contact compliance</li>
</ul>
</li>
<li>Measures of grasp performance.</li>
<li>Grasping and the kinematics of the hand</li>
</ul>
<h3 id=grasping2>Universal robotic gripper based on the jamming of granular material</h3>
<p><em>Eric Brown et. al</em><br>
<a href=http://www.pnas.org/content/107/44/18809.full.pdf><em>Source</em></a></p>
<ul>
<li>Multifindered hand in animals(or robots) require a central processor for multitude of decisions.
<img src=../../../images/papers/gripper_1.jpg alt=alt><br>
Jamming-based grippers for picking up a wide range of objects without the need for active feedback.<br>
(A) Attached to a fixed-base robot arm.<br>
(B) Picking up a shock absorber coil. <br>
(C) View from the underside. <br>
(D) Schematic of operation. <br>
(E) Holding force Fh for several three-dimensional-printed test shapes (the diameter of the sphere shown on the very left, 2R ¼ 25.4 mm, can be used for size comparison). The thin disk could not be picked up at all.</li>
</ul>
<h3 id=grasping3>Dex-Net 2.0: Deep Learning to Plan Robust Grasps with Synthetic Point Clouds and Analytic Grasp Metrics</h3>
<p><em>Jeffrey Mahler et. al</em><br>
<em>Berkley</em><br>
<a href=https://arxiv.org/abs/1703.09312><em>Source</em></a>
<a href=http://berkeleyautomation.github.io/dex-net/><em>Implementation and documentation</em></a></p>
<ul>
<li>Using physics based analyses to compute grasp configurations can be slow and error prone because of precision issues.</li>
<li>Here we use a CNN on depth images and rendered point cloud data to estimate robust claw grasping configurations.</li>
<li>Previous related work related to Grasp Planning is discussed. <br>
<img src=../../../images/papers/dexnet_1.jpg alt=alt><br>
<em>Dex-Net 2.0 Architecture. (Center) The Grasp Quality Convolutional Neural Network (GQ-CNN) is trained offline to predict the robustness candidate grasps from depth images using a dataset of 6.7 million synthetic point clouds, grasps, and associated robust grasp metrics computed with DexNet 1.0. (Left) When an object is presented to the robot, a depth camera returns a 3D point cloud, where pairs of antipodal points identify a set of several hundred grasp candidates. (Right) The GQ-CNN rapidly determines the most robust grasp candidate, which is executed with the ABB YuMi robot.</em></li>
</ul>
<p><img src=../../../images/papers/dexnet_2.jpg alt=alt><br>
<em>Graphical model for robust parallel-jaw grasping of objects on a table surface based on point clouds. Blue nodes are variables included in the state representation. Object shapes $\mathcal{O}$ are uniformly distributed over a discrete set of object models and object poses $\mathcal T_o$ are distributed over the object’s stable poses and a bounded region of a planar surface. Grasps $\mathbf{u} = (\mathbf{p}, \psi)$ are sampled uniformly from the object surface using antipodality constraints. Given the coefficient of friction $\gamma$ we evaluate an analytic success metric $S$ for a grasp on an object. A synthetic 2.5D point cloud $\mathbf y$ is generated from 3D meshes based on the camera pose $\mathcal T_c$, object shape, and pose and corrupted with multiplicative and Gaussian Process noise.</em></p>
<p><img src=../../../images/papers/dexnet_3.jpg alt=alt><br>
<em>Dex-Net 2.0 pipeline for training dataset generation. (Left) The database contains 1,500 3D object mesh models. (Top) For each object, we sample hundreds of parallel-jaw grasps to cover the surface and evaluate robust analytic grasp metrics using sampling. For each stable pose of the object we associate a set of grasps that are perpendicular to the table and collision-free for a given gripper model. (Bottom) We also render point clouds of each object in each stable pose, with the planar object pose and camera pose sampled uniformly at random. Every grasp for a given stable pose is associated with a pixel location and orientation in the rendered image. (Right) Each image is rotated, translated, cropped, and scaled to align the grasp pixel location with the image center and the grasp axis with the middle row of the image, creating a 32 × 32 grasp image. The full dataset contains over 6.7 million grasp images</em></p>
<ul>
<li>The performance of the network is compared with other machine learning models based on random forests and SVM along with previous DL based approaches.</li>
<li>Some failure modes are identified as being unable to identify thin geometries and some problems with finding collision-free grasps in narrow parts of the object geometry, suggesting that performance can be improved with better depth sensing.</li>
</ul>
<h3 id=grasping4>Learning Hand-eye Coordination for Robotic Grasping with Deep Learning and Large-scale Data Collection</h3>
<p><a href=https://people.eecs.berkeley.edu/~svlevine/papers/grasp_iser.pdf><em>Source</em></a><br>
<em>Sergey Levine, Peter Pastor, Alex Krizhevsky, and Deirdre Quillen</em><br>
<em>Google</em></p>
<ul>
<li>
<p>Hand-eye coordination for robot grasping using monocular images.</p>
</li>
<li>
<p>Consists of two components, first is a CNN to determine if a given motion would produce a successful grasp.<br>
<img src=../../../images/papers/grasp_1.jpg alt=alt><br>
<em>The architecture of our CNN grasp predictor. The input image $\mathbf{I}_t$, as well as the pregrasp image $\mathbf{I}_0$, are fed into a $6 \times 6$ convolution with stride 2, followed by $3 \times 3$ max-pooling and 6 $5 \times 5$ convolutions. This is followed by a $3 \times 3$ max-pooling layer. The motor command $v_t$ is processed by one fully connected layer, which is then pointwise added to each point in the response map of pool2 by tiling the output over the special dimensions. The result is then processed by 6 $3 \times 3$ convolutions, $2 \times 2$ max-pooling, 3 more $3 \times 3$ convolutions, and two fully connected layers with 64 units, after which the network outputs the probability of a successful grasp through a sigmoid. Each convolution is followed by batch normalization.</em></p>
</li>
<li>
<p>Second is a continuous servoing mechanism that uses CNN to continuously update the robot&rsquo;s motor channels.</p>
</li>
</ul>
<h2 id=mobile-robots>Mobile Robots</h2>
<h3 id=mobile1>Modular and Reconfigurable Mobile Robots</h3>
<p><a href=http://128.173.188.245/publications/J13_Modular_Robots_JRAS.pdf><em>Source</em></a></p>
<ul>
<li>Classification
<ul>
<li><img src=../../../images/papers/modularBots1.jpg alt=alt></li>
</ul>
</li>
<li>Modular robots with mobile configuration change(MCC)
<ul>
<li>S-bots
<ul>
<li><img src=../../../images/papers/modularBots2.jpg alt=alt></li>
</ul>
</li>
<li>Uni-Rovers
<ul>
<li><img src=../../../images/papers/modularBots3.jpg alt=alt></li>
</ul>
</li>
<li>JL-I and JL-II
<ul>
<li><img src=../../../images/papers/modularBots4.jpg alt=alt></li>
</ul>
</li>
<li>Millibots
<ul>
<li><img src=../../../images/papers/modularBots5.jpg alt=alt></li>
</ul>
</li>
<li>AMOEBA
<ul>
<li><img src=../../../images/papers/modularBots6.jpg alt=alt></li>
</ul>
</li>
</ul>
</li>
<li>Modular robots with whole body locomotion(WBL)
<ul>
<li>Whole body locomotion in chain architecture
<ul>
<li>CONRO/PolyBot
<ul>
<li><img src=../../../images/papers/modularBots7.jpg alt=alt></li>
</ul>
</li>
<li>GZ-I</li>
<li>CKBot
<ul>
<li><img src=../../../images/papers/modularBots8.jpg alt=alt></li>
</ul>
</li>
</ul>
</li>
<li>Whole body locomotion in a lattice architecture
<ul>
<li>Macro robots in a lattice architecture
<ul>
<li><em>Crystalline</em>
<ul>
<li><img src=../../../images/papers/modularBots9.jpg alt=alt></li>
</ul>
</li>
<li><em>Odin</em>
<ul>
<li><img src=../../../images/papers/modularBots10.jpg alt=alt></li>
</ul>
</li>
<li><em>I-Cubes</em>
<ul>
<li><img src=../../../images/papers/modularBots11.jpg alt=alt></li>
</ul>
</li>
<li><em>Catoms</em>
<ul>
<li><img src=../../../images/papers/modularBots12.jpg alt=alt></li>
</ul>
</li>
</ul>
</li>
<li>Mini robots in a lattice archtecture
<ul>
<li><img src=../../../images/papers/modularBots13.jpg alt=alt></li>
</ul>
</li>
<li>Reconfigurable mechanisms in a lattice architecture
<ul>
<li><img src=../../../images/papers/modularBots14.jpg alt=alt></li>
</ul>
</li>
<li>Whole body locomotion in a hybrid architecture
<ul>
<li>M-TRAN/iMobot
<ul>
<li><img src=../../../images/papers/modularBots15.jpg alt=alt></li>
</ul>
</li>
<li>Molecubes</li>
<li>ATRON
<ul>
<li><img src=../../../images/papers/modularBots16.jpg alt=alt></li>
</ul>
</li>
<li>YaMOR</li>
<li>SuperBot
<ul>
<li><img src=../../../images/papers/modularBots17.jpg alt=alt></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id=swarm-intelligence>Swarm Intelligence</h2>
<h3 id=swarm1>Swarm Intelligence and Its Applications in Swarm Robotics</h3>
<p><em>ALEKSANDAR JEVTIC, DIEGO ANDINA : Dec 2007</em><br>
<a href=http://www.wseas.us/e-library/conferences/2007tenerife/papers/572-074.pdf><em>Source</em></a></p>
<ul>
<li>An overview of swarm intelligence.<br>
<img src=../../../images/papers/swarm1.jpg alt=alt></li>
<li>Self-organizing mechanisms
<ul>
<li>Positive feedback</li>
<li>Negative feedback</li>
<li>Amplification of fluctuations</li>
<li>Multiple Interactions</li>
</ul>
</li>
<li>A <em>critical</em> number of individuals are required for <em>intelligence</em> to arise.</li>
<li><strong>Particle Swarm Optimization</strong>
<ul>
<li>Inspired by the flocking behavior of birds.</li>
<li>Composed of <em>simple</em> agents moving through multi-dimensional space, governed by a pull towards best known position to it and its neighbors.</li>
</ul>
</li>
<li><strong>Ant Colony Optimization</strong>
<ul>
<li>Uses the <em>trail-laying-trail-following</em> behavior to communicate.</li>
</ul>
</li>
<li>Applications os SI,
<ul>
<li>Every problem, application, that in its base has some kind of optimization can be tackled
with SI techniques.</li>
</ul>
</li>
<li><strong>Swarm Robotics</strong> : Inspired but not limited to SI.</li>
<li>Criteria
<ul>
<li><em>Autonomy</em> : autonomous units.</li>
<li><em>Large number</em> : large homogeneous groups</li>
<li><em>Limited capabilities</em> : incapable or inefficient on their own.</li>
<li><em>Scalability and Robustness</em> : Loosing some units should not cause a failure.</li>
<li><em>Distributed coordination</em> : Local and limited sensing and communication.</li>
</ul>
</li>
<li>Applications of SR
<ul>
<li><em>Foraging</em> : includes collective exploration, path finding, efficient task allocation and collective transport. Examples, Search-and-rescue and terrain sample collection.</li>
<li><em>Dangerous tasks</em> : Ex. mining.</li>
<li><em>Exploration and mapping</em> : Collective exploration in space, extra-terrestrial planets and human veins and arteries.</li>
</ul>
</li>
</ul>
<footer>
<h4>See also</h4>
<ul>
<li><a href=../../../blog-hugo/research/robotics/biorobots/>Biologically Inspired Robots</a></li>
<li><a href=../../../blog-hugo/research/robotics/kuka-kinematics/>Inverse Kinematics on Kuka Arm using ROS and Python</a></li>
<li><a href=../../../blog-hugo/research/robotics/research-robotics/>Robotics Research</a></li>
<li><a href=../../../blog-hugo/research/robotics/soft-robots/>Soft Robotics</a></li>
<li><a href=../../../blog-hugo/research/robotics/rsend-term2/>Robotics Engineer Nanodegree :: Term 2</a></li>
</ul>
</footer>
</article>
</div>
<aside class="col-12 col-lg-3 ml-auto blog-sidebar">
<section>
<h4>Recent Posts</h4>
<ol class=list-unstyled>
<li>
<a href=../../../blog-hugo/books/programming/the-little-schemer/>The Little Schemer</a>
</li>
<li>
<a href=../../../blog-hugo/books/programming/game-engine-architecture/>Game Engine Architecture</a>
</li>
<li>
<a href=../../../blog-hugo/books/programming/neuralnets/neural-networks-and-deep-learning-7/>Is there a simple algorithm for intelligence?, Micheal Nelson</a>
</li>
<li>
<a href=../../../blog-hugo/books/programming/neuralnets/neural-networks-and-deep-learning-6/>Deep Learning, Micheal Nelson</a>
</li>
<li>
<a href=../../../blog-hugo/books/programming/neuralnets/neural-networks-and-deep-learning-5/>Why are deep neural networks hard to train?, Micheal Nelson</a>
</li>
</ol>
</section>
<section>
<h4>Categories</h4>
<p>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/categories/books>books</a>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/categories/courses>courses</a>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/categories/machine-intelligence>machine-intelligence</a>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/categories/notes>notes</a>
</p>
<h4>Tags</h4>
<p>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/tags/algorithms>algorithms</a>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/tags/architecture>architecture</a>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/tags/biorobots>biorobots</a>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/tags/book>book</a>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/tags/data-science>data-science</a>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/tags/deep-learning>deep-learning</a>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/tags/design>design</a>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/tags/game-engine>game-engine</a>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/tags/graphics>graphics</a>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/tags/image-segmentation>image-segmentation</a>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/tags/kuka>kuka</a>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/tags/lisp>lisp</a>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/tags/machine-intelligence>machine-intelligence</a>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/tags/notes>notes</a>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/tags/opengl>opengl</a>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/tags/philosophy>philosophy</a>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/tags/programming>programming</a>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/tags/projects>projects</a>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/tags/robotics>robotics</a>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/tags/scheme>scheme</a>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/tags/style-transfer>style-transfer</a>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/tags/udacity>udacity</a>
</p>
</section>
</aside>
</div>
</div>
<script src=https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta1/dist/js/bootstrap.bundle.min.js integrity=sha384-ygbV9kiqUc6oa4msXn9868pTtWMgiQaeYH7/t7LECLbyPA2x65Kgf80OJFdroafW crossorigin=anonymous></script>
</body>
</html>