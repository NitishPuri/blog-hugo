<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
<meta http-equiv="x-ua-compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<meta property="og:title" content="Neural Network Architectures" />
<meta property="og:description" content="Deep Learning Architectures Self-Normalizing Neural Networks Günter Klambauer, Thomas Unterthiner, Andreas Mayr, Sepp Hochreiter : Sep 2017
Source
 Deep learning is setting new benchmarks everyday with the help of RNNs and CNNs. However, looking at problems that are not related to vision or sequential tasks, gradient boosting, random forests, or support vector machines are winning most of the competitions(Eg. Kaggle, HIGGS Challenge). With CNNs success, batch normalization and other stochastic regularization techniques has evolved into a standard." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://www.myproductionurl.com/research/machine-intelligence/nn-arch-1/" /><meta property="og:image" content="https://www.myproductionurl.com/images/site-feature-image.jpg"/><meta property="article:section" content="research" />

<meta property="article:modified_time" content="2021-11-09T20:19:04+05:30" />


<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://www.myproductionurl.com/images/site-feature-image.jpg"/>

<meta name="twitter:title" content="Neural Network Architectures"/>
<meta name="twitter:description" content="Deep Learning Architectures Self-Normalizing Neural Networks Günter Klambauer, Thomas Unterthiner, Andreas Mayr, Sepp Hochreiter : Sep 2017
Source
 Deep learning is setting new benchmarks everyday with the help of RNNs and CNNs. However, looking at problems that are not related to vision or sequential tasks, gradient boosting, random forests, or support vector machines are winning most of the competitions(Eg. Kaggle, HIGGS Challenge). With CNNs success, batch normalization and other stochastic regularization techniques has evolved into a standard."/>



    <link rel="canonical" href="https://www.myproductionurl.com/research/machine-intelligence/nn-arch-1/">

    <title>
      
        Neural Network Architectures | Blog - Nitish Puri
      
    </title>

    
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-giJF6kkoqNQ00vy+HMDP7azOuL0xtbfIcaT9wjKHr8RbDVddVHyTfAAsrekwKmP1" crossorigin="anonymous">
    <link href='../../../css/style.css' rel="stylesheet">

    

        
    

    <script>
MathJax = {
    tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$','$$'], ['\\[', '\\]']],
    processEscapes: true,
    processEnvironments: true
    },
    options: {
    skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
};

window.addEventListener('load', (event) => {
    document.querySelectorAll("mjx-container").forEach(function(x){
        x.parentElement.classList += 'has-jax'})
    });

</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  </head>
  <body>
    
      

<header class="blog-header">
    <nav class="navbar navbar-expand-md navbar-light bg-light">
        <div class="container-fluid">
            <a class="navbar-brand" href="../../../">
                <img src="https://getbootstrap.com/docs/4.1/assets/brand/bootstrap-solid.svg" width="30" height="30" class="d-inline-block align-top"
    alt="">
Blog - Nitish Puri
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false"
                aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse justify-content-between" id="navbarNav">
                <ul class="navbar-nav">
                    
                    
                    <li class="nav-item">
                        
                            
                            <a class="nav-link " href="../../../books/"></a>
                        
                    </li>
                    
                    <li class="nav-item">
                        
                            
                            <a class="nav-link " href="../../../research/"></a>
                        
                    </li>
                    
                    <li class="nav-item">
                        
                            
                            <a class="nav-link " href="../../../about/">About</a>
                        
                    </li>
                    
                </ul>
                
            </div>
        </div>
    </nav>
</header>

    

    
    <div class="container">
      <div class="row">
        <div class="col-12 col-lg-8 blog-main"><nav class="breadcrumb container" aria-label="breadcrumb"  >
    <ol class="breadcrumb">
    
    
    
        
    
        
    
        
    
        
    
    
        <li class="breadcrumb-item"> <a href="https://www.myproductionurl.com/">Home</a> </li>
    
    
        <li class="breadcrumb-item"> <a href="https://www.myproductionurl.com/research/">Researches</a> </li>
    
    
        <li class="breadcrumb-item"> <a href="https://www.myproductionurl.com/research/machine-intelligence/">Machine Intelligence</a> </li>
    
    
    <li class="breadcrumb-item active" aria-current="page">  Neural Network Architectures</li>
    
    
</ol>
</nav>

<header>
    <h2 class="blog-post-title">
    <a class="text-dark text-decoration-none" href="../../../research/machine-intelligence/nn-arch-1/">Neural Network Architectures</a>
</h2>

    


<div class="blog-post-date text-secondary">
    
    
        by <span rel="author">Nitish Puri</span>
    
</div>

    
<div class="blog-post-tags text-secondary">
    <strong>Tags:</strong>
    
        <a class="btn btn-primary btn-small badge" href="../../../tags/notes">notes</a>
    
        <a class="btn btn-primary btn-small badge" href="../../../tags/deep-learning">deep-learning</a>
    
</div>


    
<div class="blog-post-categories text-secondary">
    <strong>Categories:</strong>
    
        <a class="btn btn-primary btn-small badge" href="../../../categories/machine-intelligence">machine-intelligence</a>
    
</div>


    <hr>
</header>
<article class="blog-post">

    <nav id="TableOfContents">
  <ul>
    <li><a href="#deep-learning-architectures">Deep Learning Architectures</a>
      <ul>
        <li><a href="#a-nameselfnorm1-a-self-normalizing-neural-networks"><!-- raw HTML omitted --> <!-- raw HTML omitted --> Self-Normalizing Neural Networks</a></li>
        <li><a href="#a-namegeneralize1-a-understanding-deep-learning-requires-rethinking-generalization"><!-- raw HTML omitted --> <!-- raw HTML omitted --> Understanding deep learning requires rethinking generalization</a></li>
      </ul>
    </li>
  </ul>
</nav>
    
    <h2 id="deep-learning-architectures">Deep Learning Architectures</h2>
<h3 id="a-nameselfnorm1-a-self-normalizing-neural-networks"><!-- raw HTML omitted --> <!-- raw HTML omitted --> Self-Normalizing Neural Networks</h3>
<p><em>Günter Klambauer, Thomas Unterthiner, Andreas Mayr, Sepp Hochreiter : Sep 2017</em><br>
<a href="https://arxiv.org/abs/1706.02515"><em>Source</em></a></p>
<ul>
<li>Deep learning is setting new benchmarks everyday with the help of RNNs and CNNs.</li>
<li>However, looking at problems that are not related to vision or sequential tasks, gradient boosting, random forests, or support vector machines are winning most of the competitions(Eg. Kaggle, <a href="https://www.kaggle.com/c/higgs-boson">HIGGS Challenge</a>).</li>
<li>With CNNs success, batch normalization and other stochastic regularization techniques has evolved into a standard.</li>
<li>Both RNNs and CNNs can stabilize learning with weight sharing.</li>
<li>However, this is not very useful with FNNs, and often leads to high variance.</li>
<li><strong>Self-Normalizing Neural Networks</strong></li>
<li><strong>Definition 1 :</strong> *A neural network is self-normalizing if it possesses a mapping $g : \Omega \mapsto\Omega$ for each activation $y$ that maps mean and variance from one layer to the next and has a stable and attracting fixed point depending on $(\omega,\tau)$ in $\Omega$. Furthermore, the mean and the variance remain in the domain $\Omega$, that is $g(\Omega)\subseteq\Omega$, where $\Omega = {(\mu,\nu)|\mu\in[\mu_{min}, \mu{max}], \nu\in[\nu_{min}, \nu{max}]}$. When iteratively applying the mapping $g$, each point within $\Omega$ converges to this fixed point.*</li>
<li>So, SNNs keep normalization of activations when propagating them through layers of the network.</li>
<li><strong>Constructing SNNs :</strong>
<ul>
<li>The activation function, SELU
$$\text{selu}(x) = \lambda\begin{cases}x, &amp; \text{if } x \ge 0 \<br>
\alpha e^x - \alpha, &amp; \text{if } x \leq 0 \end{cases}$$</li>
<li>This activation allows to construct a mapping $g$ with properties that lead to SNNs. They cannot be derived with (scaled) ReLUs, sigmoid units, $tanh$ units and leaky ReLUs.</li>
<li>For weight initialization $\omega=0$ and $\tau=1$ for all units in higher layer is proposed.<br>
<img src="../../../images/papers/selfNeural1.jpg" alt="alt"></li>
</ul>
</li>
<li>New Dropout techniques are introduced.</li>
<li>Benchmarks compared for UCI repository datasets, outperforming FNNs with and without normalization techniques, such as batch, layer and weight normalization or specialized architectures such as ResNets.</li>
<li>Also proved that SNNs do not face vanishing and exploding gradients problem and therefore work well for architectures with many layers.</li>
<li>The best performaing SNNs are typically very deep in contrast to other FNNs.</li>
</ul>
<h3 id="a-namegeneralize1-a-understanding-deep-learning-requires-rethinking-generalization"><!-- raw HTML omitted --> <!-- raw HTML omitted --> Understanding deep learning requires rethinking generalization</h3>
<p><em>Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, Oriol Vinyals : Feb 2017</em><br>
<a href="https://arxiv.org/abs/1611.03530"><em>Source</em></a></p>
<ul>
<li>Neural networks have far more often trainable parameters than the number of samples they are trained on. Even then they exhibit small <em>generalization errorr</em> i.e. difference between &ldquo;training error&rdquo; and &ldquo;test error&rdquo;.</li>
<li><strong>Effective Capacity Of Neural Networks</strong>
<ul>
<li><strong>Randomization tests.</strong> Standard architectures were trained on a copy of data where the true labels were replaced by random labels.</li>
<li><em>Deep neural networks easily fit random labels</em> i.e. the they achieve 2 test error.</li>
<li>The test error was of course no better than random chance as there was no correlation between the training and test labels.</li>
<li>Also replacing the true images with random pixels(Gaussian noise), we observe that CNNs continue to fit the data with zero training error.
<img src="../../../images/papers/nnGeneralization1.jpg" alt="alt"></li>
<li>This has the following implications:
<ul>
<li>The effective capacity of neural networks is sufficient for memorizing the entire dataset.</li>
<li>Even optimization on random labels remains easy. In fact, training time increases only by a small constant factor compared with training on the true labels.</li>
<li>Randomizing the labels is solely a data transformation, leaving all other properties of the learning problem unchanged.</li>
</ul>
</li>
</ul>
</li>
<li><strong>The role of regularization</strong></li>
<li><em>Explicit regularization may improve generalization performance, but is neither necessary not by itself sufficient for controlling generalization error.</em></li>
<li>$l_2$ regularization sometimes even helps optimization, illustrating its poorly understoof nature in deep learning.
<img src="../../../images/papers/nnGeneralization2.jpg" alt="alt"></li>
<li><strong>Finite sample expressivity</strong></li>
<li><strong>Theorem 1.</strong> <em>There exists a two-layer neural network with ReLU activations and $2n+d$ weights that can represent any function on a sample of size $n$ in $d$ dimensions.</em></li>
<li><strong>Implicit Regularization : An Appeal To Linear Models</strong></li>
<li>Arguments showing that it is not necessarily easy to understand the source of generalization for linear models either.</li>
<li>Do all global minima generalize equally well? Is there a way to determine when one global minimum will generalize whereas another will not? In case of a linear model, even the curvature of the loss function would be the same.</li>
</ul>


    

    <footer>


    <h4>See also</h4>
    <ul>
        
            <li><a href="../../../books/programming/neuralnets/neural-networks-and-deep-learning-4/">A visual proof that neural networks can compute any function, Micheal Nelson</a></li>
        
            <li><a href="../../../books/programming/neuralnets/neural-networks-and-deep-learning-6/">Deep Learning, Micheal Nelson</a></li>
        
            <li><a href="../../../books/programming/neuralnets/neural-networks-and-deep-learning-2/">How the backpropogation algorithm works, Micheal Nelson</a></li>
        
            <li><a href="../../../books/programming/neuralnets/neural-networks-and-deep-learning-3/">Improving the way neural networks learn, Micheal Nelson</a></li>
        
            <li><a href="../../../books/programming/neuralnets/neural-networks-and-deep-learning-7/">Is there a simple algorithm for intelligence?, Micheal Nelson</a></li>
        
    </ul>

</footer>

</article>



        </div>

        <aside class="col-12 col-lg-3 ml-auto blog-sidebar">
    
        


<section>
    <h4>Recent Posts</h4>
    <ol class="list-unstyled">
        
        <li>
            <a href="../../../books/programming/the-little-schemer/">The Little Schemer</a>
        </li>
        
        <li>
            <a href="../../../books/programming/game-engine-architecture/">Game Engine Architecture</a>
        </li>
        
        <li>
            <a href="../../../books/programming/neuralnets/neural-networks-and-deep-learning-7/">Is there a simple algorithm for intelligence?, Micheal Nelson</a>
        </li>
        
        <li>
            <a href="../../../books/programming/neuralnets/neural-networks-and-deep-learning-6/">Deep Learning, Micheal Nelson</a>
        </li>
        
        <li>
            <a href="../../../books/programming/neuralnets/neural-networks-and-deep-learning-5/">Why are deep neural networks hard to train?, Micheal Nelson</a>
        </li>
        
    </ol>
</section>


    
    
        <section>
    
        
    
        
        <h4>Categories</h4>
        <p>
            
            <a class="btn btn-primary btn-small badge" href="../../../categories/books">books</a>
            
            <a class="btn btn-primary btn-small badge" href="../../../categories/courses">courses</a>
            
            <a class="btn btn-primary btn-small badge" href="../../../categories/machine-intelligence">machine-intelligence</a>
            
            <a class="btn btn-primary btn-small badge" href="../../../categories/notes">notes</a>
            
        </p>
        
    
        
        <h4>Tags</h4>
        <p>
            
            <a class="btn btn-primary btn-small badge" href="../../../tags/algorithms">algorithms</a>
            
            <a class="btn btn-primary btn-small badge" href="../../../tags/architecture">architecture</a>
            
            <a class="btn btn-primary btn-small badge" href="../../../tags/biorobots">biorobots</a>
            
            <a class="btn btn-primary btn-small badge" href="../../../tags/book">book</a>
            
            <a class="btn btn-primary btn-small badge" href="../../../tags/data-science">data-science</a>
            
            <a class="btn btn-primary btn-small badge" href="../../../tags/deep-learning">deep-learning</a>
            
            <a class="btn btn-primary btn-small badge" href="../../../tags/design">design</a>
            
            <a class="btn btn-primary btn-small badge" href="../../../tags/game-engine">game-engine</a>
            
            <a class="btn btn-primary btn-small badge" href="../../../tags/graphics">graphics</a>
            
            <a class="btn btn-primary btn-small badge" href="../../../tags/image-segmentation">image-segmentation</a>
            
            <a class="btn btn-primary btn-small badge" href="../../../tags/kuka">kuka</a>
            
            <a class="btn btn-primary btn-small badge" href="../../../tags/lisp">lisp</a>
            
            <a class="btn btn-primary btn-small badge" href="../../../tags/machine-intelligence">machine-intelligence</a>
            
            <a class="btn btn-primary btn-small badge" href="../../../tags/notes">notes</a>
            
            <a class="btn btn-primary btn-small badge" href="../../../tags/opengl">opengl</a>
            
            <a class="btn btn-primary btn-small badge" href="../../../tags/philosophy">philosophy</a>
            
            <a class="btn btn-primary btn-small badge" href="../../../tags/programming">programming</a>
            
            <a class="btn btn-primary btn-small badge" href="../../../tags/projects">projects</a>
            
            <a class="btn btn-primary btn-small badge" href="../../../tags/robotics">robotics</a>
            
            <a class="btn btn-primary btn-small badge" href="../../../tags/scheme">scheme</a>
            
            <a class="btn btn-primary btn-small badge" href="../../../tags/style-transfer">style-transfer</a>
            
            <a class="btn btn-primary btn-small badge" href="../../../tags/udacity">udacity</a>
            
        </p>
        
    
        
    
</section>

    

    

</aside>


      </div>
    </div>
    

    
      
    

    
    
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta1/dist/js/bootstrap.bundle.min.js" integrity="sha384-ygbV9kiqUc6oa4msXn9868pTtWMgiQaeYH7/t7LECLbyPA2x65Kgf80OJFdroafW" crossorigin="anonymous"></script>
  </body>
</html>
