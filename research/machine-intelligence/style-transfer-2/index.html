<!doctype html><html lang=en>
<head>
<meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="ie=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta property="og:title" content="Style Transfer, Part 2">
<meta property="og:description" content="Style Transfer Artistic style transfer for videos Manuel Ruder, Alexey Dosovitskiy, Thomas Brox : Apr 2016
Source
 The previously discussed techniques have been applied to videos on per frame basis. However, processing each frame of the video independently leads to flickering and false discontinuities, since the solution of the style transfer task is not stable. To regularize the transfer temporal constraints using optical flow are introduced.  Notation  $\mathbf p^{(i)}$ is the $i^{th}$ frame of the original video.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://nitishpuri.github.io/blog-hugo/research/machine-intelligence/style-transfer-2/"><meta property="og:image" content="https://nitishpuri.github.io/images/site-feature-image.jpg"><meta property="article:section" content="research">
<meta property="article:modified_time" content="2021-11-09T20:19:04+05:30">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://nitishpuri.github.io/images/site-feature-image.jpg">
<meta name=twitter:title content="Style Transfer, Part 2">
<meta name=twitter:description content="Style Transfer Artistic style transfer for videos Manuel Ruder, Alexey Dosovitskiy, Thomas Brox : Apr 2016
Source
 The previously discussed techniques have been applied to videos on per frame basis. However, processing each frame of the video independently leads to flickering and false discontinuities, since the solution of the style transfer task is not stable. To regularize the transfer temporal constraints using optical flow are introduced.  Notation  $\mathbf p^{(i)}$ is the $i^{th}$ frame of the original video.">
<link rel=canonical href=https://nitishpuri.github.io/blog-hugo/research/machine-intelligence/style-transfer-2/>
<title>
Style Transfer, Part 2 | Blog - Nitish Puri
</title>
<link href=https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta1/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-giJF6kkoqNQ00vy+HMDP7azOuL0xtbfIcaT9wjKHr8RbDVddVHyTfAAsrekwKmP1 crossorigin=anonymous>
<link href=../../../blog-hugo/css/style.css rel=stylesheet>
<script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:['script','noscript','style','textarea','pre']}},window.addEventListener('load',a=>{document.querySelectorAll("mjx-container").forEach(function(a){a.parentElement.classList+='has-jax'})})</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
</head>
<body>
<header class=blog-header>
<nav class="navbar navbar-expand-md navbar-light bg-light">
<div class=container-fluid>
<a class=navbar-brand href=../../../blog-hugo>
<img src=https://getbootstrap.com/docs/4.1/assets/brand/bootstrap-solid.svg width=30 height=30 class="d-inline-block align-top" alt>
Blog - Nitish Puri
</a>
<button class=navbar-toggler type=button data-bs-toggle=collapse data-bs-target=#navbarNav aria-controls=navbarNav aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span>
</button>
<div class="collapse navbar-collapse justify-content-between" id=navbarNav>
<ul class=navbar-nav>
<li class=nav-item>
<a class=nav-link href=../../../blog-hugo/blog-hugo/books/></a>
</li>
<li class=nav-item>
<a class=nav-link href=../../../blog-hugo/blog-hugo/research/></a>
</li>
<li class=nav-item>
<a class=nav-link href=../../../blog-hugo/blog-hugo/about/>About</a>
</li>
</ul>
</div>
</div>
</nav>
</header>
<div class=container>
<div class=row>
<div class="col-12 col-lg-8 blog-main"><nav class="breadcrumb container" aria-label=breadcrumb>
<ol class=breadcrumb>
<li class=breadcrumb-item> <a href=https://nitishpuri.github.io/blog-hugo/>Home</a> </li>
<li class=breadcrumb-item> <a href=https://nitishpuri.github.io/blog-hugo/research/>Researches</a> </li>
<li class=breadcrumb-item> <a href=https://nitishpuri.github.io/blog-hugo/research/machine-intelligence/>Machine Intelligence</a> </li>
<li class="breadcrumb-item active" aria-current=page> Style Transfer, Part 2</li>
</ol>
</nav>
<header>
<h2 class=blog-post-title>
<a class="text-dark text-decoration-none" href=../../../blog-hugo/research/machine-intelligence/style-transfer-2/>Style Transfer, Part 2</a>
</h2>
<div class="blog-post-date text-secondary">
by <span rel=author>Nitish Puri</span>
</div>
<div class="blog-post-tags text-secondary">
<strong>Tags:</strong>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/tags/notes>notes</a>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/tags/deep-learning>deep-learning</a>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/tags/style-transfer>style-transfer</a>
</div>
<div class="blog-post-categories text-secondary">
<strong>Categories:</strong>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/categories/machine-intelligence>machine-intelligence</a>
</div>
<hr>
</header>
<article class=blog-post>
<nav id=TableOfContents>
<ul>
<li><a href=#style-transfer>Style Transfer</a>
<ul>
<li><a href=#a-namestylevideo1-a-artistic-style-transfer-for-videos> Artistic style transfer for videos</a></li>
<li><a href=#a-namefaststyle1-a-instance-normalization-the-missing-ingredient-for-fast-stylization> Instance Normalization: The Missing Ingredient for Fast Stylization</a></li>
<li><a href=#a-namestyle3-a-perceptual-losses-for-real-time-style-transfer-and-super-resolution> Perceptual Losses for Real-Time Style Transfer and Super Resolution</a></li>
<li><a href=#a-namestyleface-a-stylizing-face-images-via-multiple-exemplars> Stylizing Face Images via Multiple Exemplars</a></li>
</ul>
</li>
</ul>
</nav>
<h2 id=style-transfer>Style Transfer</h2>
<h3 id=a-namestylevideo1-a-artistic-style-transfer-for-videos> Artistic style transfer for videos</h3>
<p><em>Manuel Ruder, Alexey Dosovitskiy, Thomas Brox : Apr 2016</em><br>
<a href=https://arxiv.org/abs/1604.08610><em>Source</em></a></p>
<ul>
<li>The previously discussed techniques have been applied to videos on per frame basis.</li>
<li>However, processing each frame of the video independently leads to flickering and false discontinuities, since the solution of the style transfer task is not stable.</li>
<li>To regularize the transfer temporal constraints using optical flow are introduced.
<img src=../../../images/papers/videoStyle1.jpg alt=alt></li>
<li>Notation
<ul>
<li>$\mathbf p^{(i)}$ is the $i^{th}$ frame of the original video.</li>
<li>$\mathbf a$ is the style image.</li>
<li>$\mathbf x^{(i)}$ are the stylized frames to be generated.</li>
<li>$\mathbf {x'}^{(i)}$ is the initialization of the style optimization algorithmat frame $i$.</li>
</ul>
</li>
<li>Short-term consistency by initialization
<ul>
<li>Most basic way to yield temporal consistency is to initialize the optimization for the frame $i+1$ with the stylized frame $i$.</li>
<li>Does not perform very well if there are moving objects in the scene, so we use optical flow.</li>
<li>$\mathbf {x'}^{(i+1)}=\omega_i^{i+1}\mathbf x^{(i)}$. Here $\omega_i^{i+1}$ denotes the function tha warps a given image using the optical flow field that was estimated between $\mathbf p^{(i)}$ and $\mathbf p^{(i+1)}$.</li>
<li><em>DeepFlow</em> and <em>EpicFlow</em>, both based on <em>Deep Matching</em> are used for optical flow estimation.</li>
</ul>
</li>
<li>Temporal consistency loss
<ul>
<li>Let $\mathbf w = (u,v)$ be the optical flow in forward direction and $\mathbf {\hat w}=(\hat u, \hat v)$ the flow in backward direction.</li>
<li>Then, $\mathbf {\tilde w}(x,y) = \mathbf{w}((x,y) + \mathbf{\hat{w}}(x,y))$ is the forward flow warped to the second image.</li>
<li>In areas without disoclusion, this warped flow should be approximately the opposite of the backward flow.</li>
<li>So, we can find the areas of disoclusions where $|\mathbf{\widetilde{w} + \hat{w}}|^2 > 0.01(|\mathbf{\widetilde{w}}|^2+|\mathbf{\hat{w}}|^2)+0.5$.</li>
<li>and motion boundaries can be detected where $|\Delta\mathbf{\hat{u}}|^2+|\Delta\mathbf{\hat{v}}|^2>0.01|\mathbf{\hat{w}}|^2+0.002$.</li>
<li>So, temporal consistency loss function penalizes deviations from the warped image in regions where the optical flow is consistent and estimated with high confidence.<br>
$$\mathcal{L}<em>{temporal}(\mathbf{x,\omega,c}) = \frac1D\sum</em>{k=1}^Dc_k\cdot(x_k-\omega_k)^2$$</li>
<li>Here, $\mathbf{c}\in [0,1]^D$ is per-pixel weighing of the loss and $D=W\times{H}\times{C}$ is the dimensionality of the image.</li>
<li>We define $\mathbf{c}^{(i-1,i)}$ between frames $i-1$ and $i$ as $0$ in disoccluded regions and the motion boundaries, and 1 everywhere else.</li>
<li>So, overall loss takes the form,<br>
$$\mathcal L_{shortterm}(\mathbf{p}^{(i)},\mathbf{a},\mathbf{x}^{(i)}) = \alpha\mathcal{L}_{content}(\mathbf{p}^{(i)},\mathbf{x}^{(i)}) + \beta\mathcal{L}_{style}(\mathbf{a},\mathbf{x}^{(i)}) + \gamma\mathcal{L}_{temporal}(\mathbf{x}^{(i)}, \omega_{i-1}^i(\mathbf{x}^{(i-1)}), \mathbf{c}^{(i-1,i)})$$</li>
</ul>
</li>
<li>Long-term consistency
<ul>
<li>The short-term model has the following limitation: when some areas are occluded in some frame and disoccluded later, these areas will likely change their appearance in the stylized video.</li>
<li>So, we need to use a penalization for deviations from more distant frames too.</li>
<li>$J$ is the set of relative indices that each frame takes into account.</li>
<li>So, the loss function is,
$$\mathcal L_{longterm}(\mathbf{p}^{(i)},\mathbf{a},\mathbf{x}^{(i)}) = \alpha\mathcal{L}_{content}(\mathbf{p}^{(i)},\mathbf{x}^{(i)}) + \beta\mathcal{L}_{style}(\mathbf{a},\mathbf{x}^{(i)}) + \gamma\sum_{j\in J:i-j\geq1}\mathcal{L}_{temporal}(\mathbf{x}^{(i)}, \omega_{i-j}^i(\mathbf{x}^{(i-j)}), \mathbf{c}_{long}^{(i-j,i)})$$</li>
<li>where, $\mathbf{c}<em>{long}^{(i-j,i)}=\text{max}(\mathbf{c}^{(i-j,i)} - \sum</em>{k\in J:i-k>i-j}\mathbf{c}^{(i-k,i)}, \mathbf{0})$</li>
</ul>
</li>
<li>Multi-pass algorithm
<ul>
<li>The image boundaries tend to have less contrast and less diversity than other areas.</li>
<li>This is not a problem for mostly static videos, but with large camera motion, these effects can creep in towards the center, which leads to lower quality images over time.</li>
<li>So, we use a multi-pass algorithm which processes the whole sequence in multiple passes and alternating directions.</li>
<li>Each pass consists of a lower number of iterations without full convergence. The sequence is run in alternating directions with each flow and blended for some number of iterations till some convergence.</li>
<li>The multi-pass algorithm can be combined with temporal consistency loss described above.</li>
<li>Achieve good results if temporal loss is disabled in several initial passes and enabled in later passes after the images had stabilized.</li>
</ul>
</li>
<li>Long term motion estimate&mldr;</li>
<li>Artifacts at image boundaries,&mldr;</li>
<li><em>Implementation</em> : <a href=https://github.com/manuelruder/artistic-videos>https://github.com/manuelruder/artistic-videos</a></li>
<li><em>Watch in action</em> : <a href=https://youtu.be/vQk_Sfl7kSc>https://youtu.be/vQk_Sfl7kSc</a></li>
</ul>
<h3 id=a-namefaststyle1-a-instance-normalization-the-missing-ingredient-for-fast-stylization> Instance Normalization: The Missing Ingredient for Fast Stylization</h3>
<p><em>Dmitry Ulyanov, Andrea Vedaldi, Victor Lempitsky : Sep 2016</em><br>
<a href=https://arxiv.org/abs/1607.08022><em>Source</em></a></p>
<p><img src=../../../images/papers/fastStyle1.jpg alt=alt><br>
<img src=../../../images/papers/fastStyle2.jpg alt=alt></p>
<ul>
<li>Learn a generator network $g(x,z)$ that can apply to a given input image $x$ the style of another $x_0$.</li>
<li>$g$ is a convolutional neural network learned from examples $x_t$ by solving<br>
$$\text{min}<em>g\frac1n\sum</em>{t=1}n\mathcal{L}(x_0, x_t, g(x_t, z_t)), \text{where }z_t \sim\mathcal{N}(0,1)$$<br>
<img src=../../../images/papers/fastStyle3.jpg alt=alt></li>
</ul>
<h3 id=a-namestyle3-a-perceptual-losses-for-real-time-style-transfer-and-super-resolution> Perceptual Losses for Real-Time Style Transfer and Super Resolution</h3>
<p><em>Johnson, Justin and Alahi, Alexandre and Fei-Fei, Li : 2016</em><br>
<a href=http://cs.stanford.edu/people/jcjohns/eccv16/><em>Source</em></a></p>
<p><img src=../../../images/papers/superStyle1.jpg alt=alt></p>
<ul>
<li>The system consists of two components : <em>image transformation network</em> $f_W$(deep resnet with encoder-decoder scheme parameterized by weights $W$) and a <em>loss network</em> $\phi$ that is used to define several <em>loss functions</em> $l_i,&mldr;l_k$.</li>
<li>The optimization problem becomes,<br>
$$W^*=\text{arg min}<em>W\mathbf{E}</em>{x,{y_i}}[\sum_{i=1}\lambda_i l_i(f_W(x), y_i)]$$</li>
<li>Uses the loss network $\phi$ to define a <em>feature reconstruction loss</em> $l_{feat}^{\phi}$ and *style reconstruction loss* $l_{style}^{\phi}$ that measures differences in content and style between images.</li>
<li><em>Simple loss functions</em> : In addition to the perceptual losses discussed above(and described earlier), two simple loss functions that depend only on low level pixel information are used.</li>
<li><em>Pixel Loss</em> : Can only be used when ground truth is available.</li>
<li><em>Total Variation Regularization</em> : to encourage spatial smoothness.</li>
<li><em>Implementation</em> : <a href=https://github.com/jcjohnson/fast-neural-style>https://github.com/jcjohnson/fast-neural-style</a><br>
<img src=../../../images/papers/superStyle2.jpg alt=alt><br>
<img src=../../../images/papers/superStyle3.jpg alt=alt><br>
<img src=../../../images/papers/superStyle4.jpg alt=alt></li>
</ul>
<h3 id=a-namestyleface-a-stylizing-face-images-via-multiple-exemplars> Stylizing Face Images via Multiple Exemplars</h3>
<p><em>Yibing Song, Linchao Bao, Shengfeng He, Qingxiong Yang, Ming-Hsuan Yang : Aug 2017</em><br>
<a href=https://arxiv.org/abs/1708.08288><em>Source</em></a></p>
<ul>
<li>Existing methods using a single exemplar lead to inaccurate results when the exemplar does not contain sufficient stylized facial components for a given photo.
<img src=../../../images/papers/faceStyle1.jpg alt=alt></li>
<li>Proposes a style transfer algorithm in which a Markov random field is used to incorporate patches from multiple exemplars. The proposed method enables the use of all stylization information from different exemplars.
<img src=../../../images/papers/faceStyle2.jpg alt=alt><br>
<img src=../../../images/papers/faceStyle3.jpg alt=alt></li>
<li>And, proposes an artifact removal methods based on an edge-preserving filter. It removes the artifacts introduced by inconsistent boundaries of local patches stylized from different exemplars.</li>
<li>In addition to visual comparison conducted by existing methods, performs quantitative evaluations using both objective and subjective metrics to demonstrate effectiveness of the proposed method.</li>
</ul>
<footer>
<h4>See also</h4>
<ul>
<li><a href=../../../blog-hugo/research/machine-intelligence/style-transfer-1/>Style Transfer, Part 1</a></li>
<li><a href=../../../blog-hugo/books/programming/neuralnets/neural-networks-and-deep-learning-4/>A visual proof that neural networks can compute any function, Micheal Nelson</a></li>
<li><a href=../../../blog-hugo/books/programming/neuralnets/neural-networks-and-deep-learning-6/>Deep Learning, Micheal Nelson</a></li>
<li><a href=../../../blog-hugo/books/programming/neuralnets/neural-networks-and-deep-learning-2/>How the backpropogation algorithm works, Micheal Nelson</a></li>
<li><a href=../../../blog-hugo/books/programming/neuralnets/neural-networks-and-deep-learning-3/>Improving the way neural networks learn, Micheal Nelson</a></li>
</ul>
</footer>
</article>
</div>
<aside class="col-12 col-lg-3 ml-auto blog-sidebar">
<section>
<h4>Recent Posts</h4>
<ol class=list-unstyled>
<li>
<a href=../../../blog-hugo/books/programming/the-little-schemer/>The Little Schemer</a>
</li>
<li>
<a href=../../../blog-hugo/books/programming/game-engine-architecture/>Game Engine Architecture</a>
</li>
<li>
<a href=../../../blog-hugo/books/programming/neuralnets/neural-networks-and-deep-learning-7/>Is there a simple algorithm for intelligence?, Micheal Nelson</a>
</li>
<li>
<a href=../../../blog-hugo/books/programming/neuralnets/neural-networks-and-deep-learning-6/>Deep Learning, Micheal Nelson</a>
</li>
<li>
<a href=../../../blog-hugo/books/programming/neuralnets/neural-networks-and-deep-learning-5/>Why are deep neural networks hard to train?, Micheal Nelson</a>
</li>
</ol>
</section>
<section>
<h4>Categories</h4>
<p>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/categories/books>books</a>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/categories/courses>courses</a>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/categories/machine-intelligence>machine-intelligence</a>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/categories/notes>notes</a>
</p>
<h4>Tags</h4>
<p>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/tags/algorithms>algorithms</a>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/tags/architecture>architecture</a>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/tags/biorobots>biorobots</a>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/tags/book>book</a>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/tags/data-science>data-science</a>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/tags/deep-learning>deep-learning</a>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/tags/design>design</a>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/tags/game-engine>game-engine</a>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/tags/graphics>graphics</a>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/tags/image-segmentation>image-segmentation</a>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/tags/kuka>kuka</a>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/tags/lisp>lisp</a>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/tags/machine-intelligence>machine-intelligence</a>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/tags/notes>notes</a>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/tags/opengl>opengl</a>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/tags/philosophy>philosophy</a>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/tags/programming>programming</a>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/tags/projects>projects</a>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/tags/robotics>robotics</a>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/tags/scheme>scheme</a>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/tags/style-transfer>style-transfer</a>
<a class="btn btn-primary btn-small badge" href=../../../blog-hugo/tags/udacity>udacity</a>
</p>
</section>
</aside>
</div>
</div>
<script src=https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta1/dist/js/bootstrap.bundle.min.js integrity=sha384-ygbV9kiqUc6oa4msXn9868pTtWMgiQaeYH7/t7LECLbyPA2x65Kgf80OJFdroafW crossorigin=anonymous></script>
</body>
</html>